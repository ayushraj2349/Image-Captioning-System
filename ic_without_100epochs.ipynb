{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sqxGzM4rSnCj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "864c9530-3f82-410c-f3d6-ac8a57e81a78",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2.0.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.1.1 sacrebleu-2.5.1\n",
            "Collecting gTTS\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from gTTS) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.11/dist-packages (from gTTS) (8.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (2025.1.31)\n",
            "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: gTTS\n",
            "Successfully installed gTTS-2.5.4\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.21.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.2 (from gradio)\n",
            "  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.21.0-py3-none-any.whl (46.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.11 ffmpy-0.5.0 gradio-5.21.0 gradio-client-1.7.2 groovy-0.1.2 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.0 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sacrebleu\n",
        "!pip install gTTS\n",
        "!pip install nltk\n",
        "!pip install tqdm\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoekKBzcSIhd",
        "outputId": "aa63d80d-a644-4549-98bf-10c5f1f4968a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dlJqXmpNTEKM",
        "outputId": "413b9fd1-d2ab-4207-c0dc-c9fcf3c07fb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import nltk\n",
        "from PIL import Image\n",
        "import sacrebleu\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download NLTK tokenizer data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Set device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "if (device == 'cuda'):\n",
        "  print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
        "  print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
        "device = torch.device(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Otx2AER8haxq",
        "outputId": "1361acd2-c52b-4e2f-de2e-7083ce836610"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTa-t9UhUbc1",
        "outputId": "f8d3f4a2-24ea-4653-8548-48d220e72144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 3006\n",
            "Total images found: 8091\n",
            "Total images found: 8091\n",
            "Total images found: 8091\n",
            "Train, val, test sizes: 6472 647 972\n"
          ]
        }
      ],
      "source": [
        "# Vocabulary class (as in the repository)\n",
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.freq_threshold = freq_threshold\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {v: k for k, v in self.itos.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer_eng(text):\n",
        "        return [tok.lower() for tok in word_tokenize(text)]\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = {}\n",
        "        idx = 4\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer_eng(sentence):\n",
        "                frequencies[word] = frequencies.get(word, 0) + 1\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokenized_text = self.tokenizer_eng(text)\n",
        "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in tokenized_text]\n",
        "\n",
        "\n",
        "# Custom Dataset with train/val/test splitting (for .txt file with CSV format)\n",
        "class FlickrDataset(Dataset):\n",
        "    def __init__(self, root_dir, captions_file, vocabulary, transform=None, split=\"train\", split_ratio=(0.80, 0.08, 0.12)):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir: Directory with images.\n",
        "            captions_file: Path to the .txt file with captions.\n",
        "                           Expected format:\n",
        "                           image,caption\n",
        "                           1000268201_693b08cb0e.jpg,A child in a pink dress is climbing up a set of stairs in an entry way .\n",
        "                           ...\n",
        "            vocabulary: A Vocabulary object.\n",
        "            transform: Image transformations.\n",
        "            split: One of 'train', 'val', or 'test'.\n",
        "            split_ratio: Tuple for train/val/test splits.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.vocabulary = vocabulary\n",
        "\n",
        "        # Read the file and split lines\n",
        "        with open(captions_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        imgs = []\n",
        "        caps = []\n",
        "        # Skip header if present (assuming first line starts with \"image\")\n",
        "        if lines[0].strip().lower().startswith(\"image\"):\n",
        "            lines = lines[1:]\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            parts = line.split(',', 1)  # split into two parts at the first comma\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            img_name = parts[0].strip()\n",
        "            caption = parts[1].strip()\n",
        "            imgs.append(img_name)\n",
        "            caps.append(caption)\n",
        "\n",
        "        # Map images to their captions\n",
        "        self.img2caps = {}\n",
        "        for img, cap in zip(imgs, caps):\n",
        "            if img not in self.img2caps:\n",
        "                self.img2caps[img] = []\n",
        "            self.img2caps[img].append(cap)\n",
        "        self.imgs = list(self.img2caps.keys())\n",
        "        print(\"Total images found:\", len(self.imgs))\n",
        "\n",
        "        # Split dataset into train/val/test\n",
        "        random.seed(42)\n",
        "        random.shuffle(self.imgs)\n",
        "        total = len(self.imgs)\n",
        "        train_end = int(split_ratio[0] * total)\n",
        "        val_end = train_end + int(split_ratio[1] * total)\n",
        "\n",
        "        if split == \"train\":\n",
        "            self.imgs = self.imgs[:train_end]\n",
        "        elif split == \"val\":\n",
        "            self.imgs = self.imgs[train_end:val_end]\n",
        "        elif split == \"test\":\n",
        "            self.imgs = self.imgs[val_end:]\n",
        "        else:\n",
        "            raise Exception(\"split must be one of 'train', 'val', or 'test'\")\n",
        "\n",
        "        self.split = split\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_id = self.imgs[index]\n",
        "        caps = self.img2caps[img_id]\n",
        "        # For training, pick a random caption; for validation/testing, use the first caption\n",
        "        caption = random.choice(caps) if self.split == \"train\" else caps[0]\n",
        "        img_path = os.path.join(self.root_dir, img_id)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        # Add start and end tokens\n",
        "        numericalized_caption = [self.vocabulary.stoi[\"<SOS>\"]]\n",
        "        numericalized_caption += self.vocabulary.numericalize(caption)\n",
        "        numericalized_caption.append(self.vocabulary.stoi[\"<EOS>\"])\n",
        "        return image, torch.tensor(numericalized_caption)\n",
        "\n",
        "# Collate function to pad sequences\n",
        "def collate_fn(batch):\n",
        "    images, captions = zip(*batch)\n",
        "    images = torch.stack(images, 0)\n",
        "    captions = pad_sequence(captions, batch_first=True, padding_value=0)\n",
        "    return images, captions\n",
        "\n",
        "# InceptionV3 expects 299x299 images.\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Build vocabulary from the captions file (skip header if present)\n",
        "def build_vocab(captions_file, freq_threshold):\n",
        "    with open(captions_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    # Skip header line if it starts with \"image\"\n",
        "    if lines[0].strip().lower().startswith(\"image\"):\n",
        "        lines = lines[1:]\n",
        "    captions = []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        parts = line.split(',', 1)\n",
        "        if len(parts) < 2:\n",
        "            continue\n",
        "        captions.append(parts[1].strip())\n",
        "    vocab = Vocabulary(freq_threshold)\n",
        "    vocab.build_vocabulary(captions)\n",
        "    return vocab\n",
        "\n",
        "# Update these paths to your dataset locations\n",
        "captions_path = \"/content/drive/MyDrive/Flickr/captions.txt\"\n",
        "images_root = \"/content/drive/MyDrive/Flickr/Images\"\n",
        "\n",
        "# Build vocabulary (adjust frequency threshold as needed)\n",
        "vocab = build_vocab(captions_path, freq_threshold=5)\n",
        "print(\"Vocabulary size:\", len(vocab))\n",
        "num_workers = 0\n",
        "\n",
        "# Create dataset objects for train, validation, and test splits\n",
        "train_dataset = FlickrDataset(root_dir=images_root, captions_file=captions_path, vocabulary=vocab, transform=transform, split=\"train\")\n",
        "val_dataset   = FlickrDataset(root_dir=images_root, captions_file=captions_path, vocabulary=vocab, transform=transform, split=\"val\")\n",
        "test_dataset  = FlickrDataset(root_dir=images_root, captions_file=captions_path, vocabulary=vocab, transform=transform, split=\"test\")\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn, num_workers=num_workers)\n",
        "val_loader   = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)\n",
        "test_loader  = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)\n",
        "\n",
        "print(\"Train, val, test sizes:\", len(train_dataset), len(val_dataset), len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx,img_name in enumerate(test_dataset.imgs):\n",
        "    if idx==20:\n",
        "      break\n",
        "    else:\n",
        "      print(img_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epxPLEYMWVEj",
        "outputId": "bf4361e2-88ed-4f74-e082-e8479a210730"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2363419943_717e6b119d.jpg\n",
            "754852108_72f80d421f.jpg\n",
            "1295669416_21cabf594d.jpg\n",
            "3542418447_7c337360d6.jpg\n",
            "135235570_5698072cd4.jpg\n",
            "3375134059_7e9eb2ef01.jpg\n",
            "358607894_5abb1250d3.jpg\n",
            "1514957266_a19827c538.jpg\n",
            "3372340429_91c4f4af30.jpg\n",
            "2422018883_336519b5c6.jpg\n",
            "2757803246_8aa3499d26.jpg\n",
            "1419286010_b59af3962a.jpg\n",
            "707941195_4386109029.jpg\n",
            "3534548254_7bee952a0e.jpg\n",
            "1176580356_9810d877bf.jpg\n",
            "3053415073_5b667230ed.jpg\n",
            "1691573772_1adef8e40e.jpg\n",
            "3679341667_936769fd0c.jpg\n",
            "2490365757_b869282cb3.jpg\n",
            "2937497894_e3664a9513.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "m0Qgktmag0nF"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import Inception_V3_Weights\n",
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        # Instantiate InceptionV3 with the default weights and aux_logits=True (required by the weights API)\n",
        "        self.inception = models.inception_v3(weights=Inception_V3_Weights.DEFAULT, aux_logits=True)\n",
        "        # Replace the final fully connected layer with an identity so that we get features directly\n",
        "        self.inception.fc = nn.Identity()\n",
        "        # Now define our own linear layer to map the 2048-dim features to embed_size\n",
        "        self.linear = nn.Linear(2048, embed_size)\n",
        "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
        "\n",
        "    def forward(self, images):\n",
        "        # In training mode, inception returns a tuple (main_output, aux_output)\n",
        "        if self.training:\n",
        "            x, _ = self.inception(images)\n",
        "        else:\n",
        "            x = self.inception(images)\n",
        "        # Flatten the features and pass through our linear+BN layers\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.bn(self.linear(x))\n",
        "        return x\n",
        "\n",
        "# Decoder: LSTM-based decoder for caption generation (following the repository)\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        # Remove the last token (<EOS>) from the caption for input (we add <SOS> manually)\n",
        "        embeddings = self.embed(captions[:, :-1])\n",
        "        # Concatenate image features as the first token in the sequence\n",
        "        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
        "        hiddens, _ = self.lstm(embeddings)\n",
        "        outputs = self.linear(hiddens)\n",
        "        return outputs\n",
        "\n",
        "    def sample_beam(self, features, beam_size=3, max_len=20):\n",
        "      \"\"\"\n",
        "      Beam search decoding for a single image.\n",
        "      Uses the image features as the first input (as in greedy decoding).\n",
        "      Returns the best sequence (list of token ids).\n",
        "      \"\"\"\n",
        "      k = beam_size\n",
        "      vocab_size = self.linear.out_features\n",
        "\n",
        "      # Initialize the LSTM with the image features (same as in greedy decoding)\n",
        "      inputs = features.unsqueeze(1)  # shape: (1, 1, embed_size)\n",
        "      output, states = self.lstm(inputs, None)\n",
        "      outputs = self.linear(output.squeeze(1))  # shape: (1, vocab_size)\n",
        "      log_probs = torch.log_softmax(outputs, dim=1)\n",
        "      topk_log_probs, topk_indices = log_probs.topk(k)  # (1, k)\n",
        "\n",
        "      # Initialize beam candidates:\n",
        "      sequences = []\n",
        "      for i in range(k):\n",
        "          token = topk_indices[0, i].unsqueeze(0)  # initial predicted token\n",
        "          score = topk_log_probs[0, i].item()\n",
        "          sequences.append((token, score, states))\n",
        "\n",
        "      # Run decoding for subsequent time steps:\n",
        "      for t in range(max_len - 1):\n",
        "          all_candidates = []\n",
        "          for seq, score, states in sequences:\n",
        "              # If last token is <EOS>, keep the sequence as is\n",
        "              if seq[-1].item() == vocab.stoi[\"<EOS>\"]:\n",
        "                  all_candidates.append((seq, score, states))\n",
        "                  continue\n",
        "\n",
        "              # Get the embedding for the last token and run one LSTM step\n",
        "              last_token = seq[-1].unsqueeze(0)  # shape: (1,)\n",
        "              emb = self.embed(last_token).unsqueeze(1)  # (1, 1, embed_size)\n",
        "              output, new_states = self.lstm(emb, states)\n",
        "              outputs = self.linear(output.squeeze(1))  # (1, vocab_size)\n",
        "              log_probs = torch.log_softmax(outputs, dim=1)\n",
        "              topk_log_probs, topk_indices = log_probs.topk(k)\n",
        "\n",
        "              # Expand each candidate with the top k tokens:\n",
        "              for i in range(k):\n",
        "                  new_token = topk_indices[0, i].unsqueeze(0)\n",
        "                  new_seq = torch.cat([seq, new_token])\n",
        "                  new_score = score + topk_log_probs[0, i].item()\n",
        "                  all_candidates.append((new_seq, new_score, new_states))\n",
        "\n",
        "          # Order all candidates by score and select best k sequences:\n",
        "          ordered = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)\n",
        "          sequences = ordered[:k]\n",
        "\n",
        "          # If all sequences end with <EOS>, stop early.\n",
        "          if all(seq[-1].item() == vocab.stoi[\"<EOS>\"] for seq, _, _ in sequences):\n",
        "              break\n",
        "\n",
        "      best_seq, best_score, _ = sequences[0]\n",
        "      return best_seq.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8qGXzHing2-Q"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, criterion, data_loader, device):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(data_loader, desc=\"Evaluating\", leave=False)\n",
        "        for images, captions in pbar:\n",
        "            images, captions = images.to(device), captions.to(device)\n",
        "            features = encoder(images)\n",
        "            outputs = decoder(features, captions)\n",
        "            loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1))\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix({\"val_loss\": loss.item()})\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    return avg_loss\n",
        "\n",
        "def train(encoder, decoder, criterion, optimizer, train_loader, val_loader, num_epochs, device, save_path=\"Downloads/Flickr/best_model.pth\"):\n",
        "    encoder.to(device)\n",
        "    decoder.to(device)\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(num_epochs):\n",
        "        encoder.train()\n",
        "        decoder.train()\n",
        "        running_loss = 0.0\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
        "        for images, captions in pbar:\n",
        "            images, captions = images.to(device), captions.to(device)\n",
        "            features = encoder(images)\n",
        "            outputs = decoder(features, captions)\n",
        "            loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            pbar.set_postfix({\"Loss\": loss.item()})\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        avg_val_loss = evaluate(encoder, decoder, criterion, val_loader, device)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Save checkpoint if validation loss decreases\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save({\n",
        "                'encoder_state_dict': encoder.state_dict(),\n",
        "                'decoder_state_dict': decoder.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'epoch': epoch,\n",
        "                'val_loss': avg_val_loss,\n",
        "            }, save_path)\n",
        "            print(f\"Checkpoint saved at epoch {epoch+1} with val loss {avg_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WysMYivCg5JR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71976d09-1d65-47c4-bfcc-8fa5ea52d1b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Loss: 4.7781, Val Loss: 3.8086\n",
            "Checkpoint saved at epoch 1 with val loss 3.8086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/100], Train Loss: 3.5861, Val Loss: 3.4001\n",
            "Checkpoint saved at epoch 2 with val loss 3.4001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/100], Train Loss: 3.3098, Val Loss: 3.1964\n",
            "Checkpoint saved at epoch 3 with val loss 3.1964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/100], Train Loss: 3.1351, Val Loss: 3.0763\n",
            "Checkpoint saved at epoch 4 with val loss 3.0763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/100], Train Loss: 3.0282, Val Loss: 2.9906\n",
            "Checkpoint saved at epoch 5 with val loss 2.9906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/100], Train Loss: 2.9370, Val Loss: 2.9363\n",
            "Checkpoint saved at epoch 6 with val loss 2.9363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/100], Train Loss: 2.8721, Val Loss: 2.8834\n",
            "Checkpoint saved at epoch 7 with val loss 2.8834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/100], Train Loss: 2.7977, Val Loss: 2.8232\n",
            "Checkpoint saved at epoch 8 with val loss 2.8232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/100], Train Loss: 2.7596, Val Loss: 2.7850\n",
            "Checkpoint saved at epoch 9 with val loss 2.7850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Train Loss: 2.7031, Val Loss: 2.7433\n",
            "Checkpoint saved at epoch 10 with val loss 2.7433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/100], Train Loss: 2.6590, Val Loss: 2.7105\n",
            "Checkpoint saved at epoch 11 with val loss 2.7105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/100], Train Loss: 2.6152, Val Loss: 2.6740\n",
            "Checkpoint saved at epoch 12 with val loss 2.6740\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/100], Train Loss: 2.5828, Val Loss: 2.6502\n",
            "Checkpoint saved at epoch 13 with val loss 2.6502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/100], Train Loss: 2.5435, Val Loss: 2.6472\n",
            "Checkpoint saved at epoch 14 with val loss 2.6472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/100], Train Loss: 2.5073, Val Loss: 2.6131\n",
            "Checkpoint saved at epoch 15 with val loss 2.6131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/100], Train Loss: 2.4779, Val Loss: 2.6008\n",
            "Checkpoint saved at epoch 16 with val loss 2.6008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/100], Train Loss: 2.4562, Val Loss: 2.5751\n",
            "Checkpoint saved at epoch 17 with val loss 2.5751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/100], Train Loss: 2.4242, Val Loss: 2.5613\n",
            "Checkpoint saved at epoch 18 with val loss 2.5613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/100], Train Loss: 2.4044, Val Loss: 2.5543\n",
            "Checkpoint saved at epoch 19 with val loss 2.5543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/100], Train Loss: 2.3707, Val Loss: 2.5406\n",
            "Checkpoint saved at epoch 20 with val loss 2.5406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [21/100], Train Loss: 2.3548, Val Loss: 2.5228\n",
            "Checkpoint saved at epoch 21 with val loss 2.5228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [22/100], Train Loss: 2.3270, Val Loss: 2.5148\n",
            "Checkpoint saved at epoch 22 with val loss 2.5148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [23/100], Train Loss: 2.2985, Val Loss: 2.5127\n",
            "Checkpoint saved at epoch 23 with val loss 2.5127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [24/100], Train Loss: 2.2872, Val Loss: 2.4989\n",
            "Checkpoint saved at epoch 24 with val loss 2.4989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [25/100], Train Loss: 2.2671, Val Loss: 2.4971\n",
            "Checkpoint saved at epoch 25 with val loss 2.4971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [26/100], Train Loss: 2.2439, Val Loss: 2.4768\n",
            "Checkpoint saved at epoch 26 with val loss 2.4768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [27/100], Train Loss: 2.2248, Val Loss: 2.4802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [28/100], Train Loss: 2.2093, Val Loss: 2.4760\n",
            "Checkpoint saved at epoch 28 with val loss 2.4760\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [29/100], Train Loss: 2.1933, Val Loss: 2.4715\n",
            "Checkpoint saved at epoch 29 with val loss 2.4715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [30/100], Train Loss: 2.1654, Val Loss: 2.4618\n",
            "Checkpoint saved at epoch 30 with val loss 2.4618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [31/100], Train Loss: 2.1576, Val Loss: 2.4574\n",
            "Checkpoint saved at epoch 31 with val loss 2.4574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [32/100], Train Loss: 2.1222, Val Loss: 2.4559\n",
            "Checkpoint saved at epoch 32 with val loss 2.4559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [33/100], Train Loss: 2.1181, Val Loss: 2.4583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [34/100], Train Loss: 2.1061, Val Loss: 2.4486\n",
            "Checkpoint saved at epoch 34 with val loss 2.4486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [35/100], Train Loss: 2.0955, Val Loss: 2.4424\n",
            "Checkpoint saved at epoch 35 with val loss 2.4424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [36/100], Train Loss: 2.0650, Val Loss: 2.4344\n",
            "Checkpoint saved at epoch 36 with val loss 2.4344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [37/100], Train Loss: 2.0505, Val Loss: 2.4325\n",
            "Checkpoint saved at epoch 37 with val loss 2.4325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [38/100], Train Loss: 2.0331, Val Loss: 2.4298\n",
            "Checkpoint saved at epoch 38 with val loss 2.4298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [39/100], Train Loss: 2.0280, Val Loss: 2.4288\n",
            "Checkpoint saved at epoch 39 with val loss 2.4288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [40/100], Train Loss: 2.0163, Val Loss: 2.4265\n",
            "Checkpoint saved at epoch 40 with val loss 2.4265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [41/100], Train Loss: 1.9970, Val Loss: 2.4310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [42/100], Train Loss: 1.9834, Val Loss: 2.4253\n",
            "Checkpoint saved at epoch 42 with val loss 2.4253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [43/100], Train Loss: 1.9675, Val Loss: 2.4219\n",
            "Checkpoint saved at epoch 43 with val loss 2.4219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [44/100], Train Loss: 1.9541, Val Loss: 2.4209\n",
            "Checkpoint saved at epoch 44 with val loss 2.4209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [45/100], Train Loss: 1.9268, Val Loss: 2.4184\n",
            "Checkpoint saved at epoch 45 with val loss 2.4184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [46/100], Train Loss: 1.9209, Val Loss: 2.4194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [47/100], Train Loss: 1.9165, Val Loss: 2.4244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [48/100], Train Loss: 1.8985, Val Loss: 2.4265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [49/100], Train Loss: 1.8864, Val Loss: 2.4235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/100], Train Loss: 1.8723, Val Loss: 2.4356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [51/100], Train Loss: 1.8633, Val Loss: 2.4274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [52/100], Train Loss: 1.8378, Val Loss: 2.4259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [53/100], Train Loss: 1.8298, Val Loss: 2.4272\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [54/100], Train Loss: 1.8191, Val Loss: 2.4263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [55/100], Train Loss: 1.8037, Val Loss: 2.4340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [56/100], Train Loss: 1.7912, Val Loss: 2.4326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [57/100], Train Loss: 1.7856, Val Loss: 2.4298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [58/100], Train Loss: 1.7668, Val Loss: 2.4378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [59/100], Train Loss: 1.7655, Val Loss: 2.4405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [60/100], Train Loss: 1.7457, Val Loss: 2.4416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [61/100], Train Loss: 1.7436, Val Loss: 2.4391\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [62/100], Train Loss: 1.7226, Val Loss: 2.4421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [63/100], Train Loss: 1.7125, Val Loss: 2.4549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [64/100], Train Loss: 1.7049, Val Loss: 2.4582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [65/100], Train Loss: 1.6868, Val Loss: 2.4591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [66/100], Train Loss: 1.6805, Val Loss: 2.4489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [67/100], Train Loss: 1.6635, Val Loss: 2.4546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [68/100], Train Loss: 1.6574, Val Loss: 2.4577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [69/100], Train Loss: 1.6441, Val Loss: 2.4626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [70/100], Train Loss: 1.6349, Val Loss: 2.4683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [71/100], Train Loss: 1.6231, Val Loss: 2.4733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [72/100], Train Loss: 1.6097, Val Loss: 2.4803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [73/100], Train Loss: 1.5982, Val Loss: 2.4831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [74/100], Train Loss: 1.5821, Val Loss: 2.4825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [75/100], Train Loss: 1.5740, Val Loss: 2.4815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [76/100], Train Loss: 1.5645, Val Loss: 2.4916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [77/100], Train Loss: 1.5520, Val Loss: 2.4979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [78/100], Train Loss: 1.5501, Val Loss: 2.5077\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [79/100], Train Loss: 1.5359, Val Loss: 2.4967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [80/100], Train Loss: 1.5338, Val Loss: 2.5003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [81/100], Train Loss: 1.5135, Val Loss: 2.5074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [82/100], Train Loss: 1.5035, Val Loss: 2.5228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [83/100], Train Loss: 1.4980, Val Loss: 2.5236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [84/100], Train Loss: 1.4946, Val Loss: 2.5196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [85/100], Train Loss: 1.4854, Val Loss: 2.5221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [86/100], Train Loss: 1.4657, Val Loss: 2.5403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [87/100], Train Loss: 1.4523, Val Loss: 2.5315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [88/100], Train Loss: 1.4528, Val Loss: 2.5343\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [89/100], Train Loss: 1.4417, Val Loss: 2.5441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [90/100], Train Loss: 1.4204, Val Loss: 2.5574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [91/100], Train Loss: 1.4194, Val Loss: 2.5608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [92/100], Train Loss: 1.4120, Val Loss: 2.5652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [93/100], Train Loss: 1.4050, Val Loss: 2.5639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [94/100], Train Loss: 1.4018, Val Loss: 2.5622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [95/100], Train Loss: 1.3910, Val Loss: 2.5846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [96/100], Train Loss: 1.3790, Val Loss: 2.5774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [97/100], Train Loss: 1.3696, Val Loss: 2.5953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [98/100], Train Loss: 1.3598, Val Loss: 2.5948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [99/100], Train Loss: 1.3406, Val Loss: 2.6028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                                                       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/100], Train Loss: 1.3490, Val Loss: 2.5946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "# Hyperparameters (matching the original repo)\n",
        "embed_size    = 256\n",
        "hidden_size   = 512\n",
        "num_layers    = 1\n",
        "learning_rate = 3e-4\n",
        "num_epochs    = 100\n",
        "vocab_size    = len(vocab)\n",
        "\n",
        "# Initialize encoder and decoder models\n",
        "encoder = EncoderCNN(embed_size)\n",
        "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "# Define loss (ignoring the <PAD> token) and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])\n",
        "# Update only the decoder and the newly added layers of the encoder\n",
        "params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
        "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
        "\n",
        "# Start training\n",
        "train(encoder, decoder, criterion, optimizer, train_loader, val_loader, num_epochs, device, save_path=\"Downloads/Flickr/best_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters (matching the original repo)\n",
        "embed_size    = 256\n",
        "hidden_size   = 512\n",
        "num_layers    = 1\n",
        "learning_rate = 3e-4\n",
        "num_epochs    = 100\n",
        "vocab_size    = len(vocab)\n",
        "\n",
        "# Initialize encoder and decoder models\n",
        "encoder = EncoderCNN(embed_size)\n",
        "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "# Cell 7: Load the best model checkpoint and evaluate sacreBLEU score on the test set\n",
        "\n",
        "# Load checkpoint\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/Flickr/best_model_100.pth\", map_location=torch.device('cpu'))\n",
        "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
        "encoder.to(device)\n",
        "decoder.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0mHkOhzlwzl",
        "outputId": "58e4e1c6-6438-47ec-ede2-67c610687d1e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
            "100%|██████████| 104M/104M [00:00<00:00, 122MB/s] \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecoderRNN(\n",
              "  (embed): Embedding(3006, 256)\n",
              "  (lstm): LSTM(256, 512, batch_first=True)\n",
              "  (linear): Linear(in_features=512, out_features=3006, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Folder path containing test images\n",
        "folder_path = \"/content/drive/MyDrive/Flickr/test_images/\"\n",
        "\n",
        "# Dictionary to store captions for each file\n",
        "captions_dict = {}\n",
        "\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "            image_path = os.path.join(folder_path, filename)\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "            features = encoder(image_tensor)\n",
        "            # Use beam search for caption generation; you can switch to greedy with decoder.sample(features)\n",
        "            best_seq = decoder.sample_beam(features, beam_size=1, max_len=20)\n",
        "\n",
        "            # Convert token IDs to words, filtering out special tokens\n",
        "            caption_words = []\n",
        "            for token_id in best_seq:\n",
        "                word = vocab.itos[token_id]\n",
        "                if word == \"<EOS>\":\n",
        "                    break\n",
        "                if word in [\"<SOS>\", \"<UNK>\"]:\n",
        "                    continue\n",
        "                caption_words.append(word)\n",
        "            caption = \" \".join(caption_words)\n",
        "            captions_dict[filename] = caption\n",
        "            print(f\"{filename}: {caption}\")\n",
        "\n",
        "# Optionally, captions_dict now holds the mapping for further processing."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjTzsu3UEe6b",
        "outputId": "41320839-686a-4d79-e269-e2ec908553c4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boat.png: a man is rowing a canoe down a river .\n",
            "bus.png: a man in a black jacket and white hat is standing in front of a store .\n",
            "child.jpg: a little girl in a pink dress is running through a field .\n",
            "dog.jpg: a dog is running through the water .\n",
            "horse.png: a man and a dog are standing on a rocky shore .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from gtts import gTTS\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Load Trained Model Checkpoint\n",
        "# ----------------------------------------------------------------\n",
        "checkpoint_path = \"/content/drive/MyDrive/Flickr/best_model_100.pth\"\n",
        "embed_size    = 256\n",
        "hidden_size   = 512\n",
        "num_layers    = 1\n",
        "vocab_size    = len(vocab)\n",
        "\n",
        "encoder = EncoderCNN(embed_size)\n",
        "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "encoder.eval()\n",
        "decoder.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EXqbiKSeFoZ",
        "outputId": "799d0ec4-8ef6-4191-a3f7-8f154a787658"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecoderRNN(\n",
              "  (embed): Embedding(3006, 256)\n",
              "  (lstm): LSTM(256, 512, batch_first=True)\n",
              "  (linear): Linear(in_features=512, out_features=3006, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from gtts import gTTS\n",
        "import base64\n",
        "from io import BytesIO\n",
        "\n",
        "# [Your existing model loading code here...]\n",
        "\n",
        "def generate_captions_and_audio(files, beam_size, max_len=20):\n",
        "    \"\"\"Generate captions and audio for multiple images, returning HTML with embedded content\"\"\"\n",
        "    if not isinstance(files, list):\n",
        "        files = [files]\n",
        "\n",
        "    html_output = \"\"\n",
        "    with torch.no_grad():\n",
        "        for idx, file_path in enumerate(files):\n",
        "            # Process image\n",
        "            image = Image.open(file_path).convert(\"RGB\")\n",
        "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "            features = encoder(image_tensor)\n",
        "\n",
        "            # Generate caption\n",
        "            token_ids = decoder.sample_beam(features, beam_size=beam_size, max_len=max_len)\n",
        "            caption_words = []\n",
        "            for token_id in token_ids:\n",
        "                word = vocab.itos[token_id]\n",
        "                if word == \"<EOS>\": break\n",
        "                if word not in [\"<SOS>\", \"<UNK>\"]:\n",
        "                    caption_words.append(word)\n",
        "            caption = \" \".join(caption_words)\n",
        "\n",
        "            # Generate audio\n",
        "            tts = gTTS(text=caption, lang=\"en\")\n",
        "            audio_bytes = BytesIO()\n",
        "            tts.write_to_fp(audio_bytes)\n",
        "            audio_bytes.seek(0)\n",
        "            audio_b64 = base64.b64encode(audio_bytes.read()).decode()\n",
        "\n",
        "            # Convert image to base64\n",
        "            buffered = BytesIO()\n",
        "            image.save(buffered, format=\"JPEG\")\n",
        "            img_b64 = base64.b64encode(buffered.getvalue()).decode()\n",
        "\n",
        "            # Create HTML block\n",
        "            html_output += f\"\"\"\n",
        "                        <div style=\"margin: 1rem; padding: 1rem; border: 1px solid #ddd; border-radius: 8px;\">\n",
        "                            <div style=\"display: flex; gap: 1rem; align-items: center;\">\n",
        "                                <img src=\"data:image/jpeg;base64,{img_b64}\" style=\"max-width: 200px; height: auto;\"/>\n",
        "                                <div>\n",
        "                                    <p style=\"font-size: 18px; margin: 0.5rem 0;\">\n",
        "                                        <strong style=\"font-size: 16px;\">Caption:</strong>\n",
        "                                        <span style=\"font-size: 16px;\">{caption}</span>\n",
        "                                    </p>\n",
        "                                    <audio controls style=\"margin-top: 0.5rem;\">\n",
        "                                        <source src=\"data:audio/mpeg;base64,{audio_b64}\" type=\"audio/mpeg\">\n",
        "                                    </audio>\n",
        "                                </div>\n",
        "                            </div>\n",
        "                      </div>\n",
        "                      \"\"\"\n",
        "\n",
        "    return f\"<div style='margin: 1rem;'>{html_output}</div>\"\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(title=\"Image Captioning with Audio\") as interface:\n",
        "    gr.Markdown(\"# Image Captioning with Audio\")\n",
        "    gr.Markdown(\"Upload Images to Generate Captions with Playable Audio. (Up to 10 Images)\")\n",
        "    gr.Markdown(\"Model used is InceptionV3 and Training Dataset is Flickr8k.\")\n",
        "    gr.Markdown(\"Beam Search is used as the Decoding Strategy.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        inputs = [\n",
        "            gr.File(label=\"Upload Images\", file_count=\"multiple\", file_types=[\"image\"], type=\"filepath\"),\n",
        "            gr.Slider(1, 10, step=1, value=3, label=\"Beam Size\"),\n",
        "            gr.Slider(10, 50, step=1, value=20, label=\"Max Caption Length\")\n",
        "        ]\n",
        "        submit = gr.Button(\"Generate\", variant=\"primary\")\n",
        "\n",
        "    output = gr.HTML(label=\"Results\")\n",
        "\n",
        "    # Validation function\n",
        "    def validate_files(files):\n",
        "        if len(files) < 1:\n",
        "            raise gr.Error(\"Please upload at least 1 image!\")\n",
        "        if len(files) > 10:\n",
        "            raise gr.Error(\"Maximum 10 images allowed!\")\n",
        "        return files\n",
        "\n",
        "        # Add validation to file input\n",
        "    inputs[0].upload(\n",
        "        validate_files,\n",
        "        inputs[0],\n",
        "        inputs[0]\n",
        "    )\n",
        "\n",
        "    submit.click(\n",
        "        generate_captions_and_audio,\n",
        "        inputs=inputs,\n",
        "        outputs=output\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "DpjSvbTNpHUK",
        "outputId": "b3c4fafc-068a-4970-f70c-f45f29bd77fd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d75a1f555efcf7e8fb.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d75a1f555efcf7e8fb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra Part"
      ],
      "metadata": {
        "id": "SfbnRDQTgui1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gtts import gTTS\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# Suppose 'caption' is the generated caption from your inference cell.\n",
        "# If it's not defined, set it manually for testing.\n",
        "# caption = \"A child in a pink dress is climbing up a set of stairs in an entry way.\"\n",
        "\n",
        "# Convert text to speech\n",
        "tts = gTTS(text=caption, lang='en')\n",
        "\n",
        "# Save the audio file\n",
        "audio_filename = \"caption_audio.mp3\"\n",
        "tts.save(audio_filename)\n",
        "\n",
        "# Play the audio in the notebook\n",
        "display(Audio(audio_filename, autoplay=True))"
      ],
      "metadata": {
        "id": "fZAHDWdDhRtR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "c2040d42-1be6-4a08-c89d-5d3be0853500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ],
            "text/html": [
              "\n",
              "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
              "                    <source src=\"data:audio/mpeg;base64,//OExAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA//OExAAlSaYIAUkwAUaMjC4JgDAGG5EZOjbnOaNGgYhkFECAkAYDJk7v9ru7v/xd/xHeyBCIiCBAgAwGTJkyZMmTIECBAgQIECBMmTJg4DCyZMIIECCERCBMmTJk7DwwAAAAADw8PDwwAAAAADw8PD/wADMA8PDw8MAHfADw8PDx2Bn+Hh4e/QAAAAA2Hh/hgAZ///+kd/+Hh4f+gAAABAeHqpReU1MTJQUFBqpl5j4GWTMLaDSgbmZceLGjNhjR//OExB0tDCZoAZuoAQJvAkMwMDAYTADfIPAy2iQQgndwzAbeOYNMDIwZAEGgEQGAog/kEWRApm4CQGVxBEV0T9+mpBzMg4jwYw+Q4tCwK/FKHsuGhcEVGTG2QApEaYf88bnzA0QQY8PIdAIJixjLlA8TJK//l9M+aU03Tow1QLGLUMaRQc9EvIkWY3//////v/oGFv///u///zxfOpGSP9mND4fVqzkaQFCB+K0LFZyYhk8eJHzWmNJhmEpY8NMS//OExBsq8zaEAdugAQqGGLBJnD4YsDtNL2oABAQCQC+IN4gbPACDwcDBoECx4UkBpAAZULnRH4XVgYMCVhZpmbiy0ykXigMg5wyOEmk50skKcSOnCrZRSJYzNCbICMU2NioTiaTJnzRqToVIop3MyYJ8nzckimbMcSUykkDBaSVNjMnVlkwJl0zc3MnOpUXbu3o/////////qPJKLiG7hDachi8YA1SFjIayw0THJvNKxmwWCjejgOIkcoeTIOBi//OExCIv61p8AN7WvEiFJZLxwKM8ZXBlElBI6dCbM6cRv4AGxpIpNdOYCm5QzMWv7Z6UBdLlXfZqFa/Nu/B9XlAsSeu626fKmMw2P8Jxua1tT0ipGP36ZiY+MqFSw50PB48T2A8PADBgN4fIcgqem7OKvq4JxspZ5I2NFh8HkdA6wIyOIFzDyE1rxP////////8//3////f8d9Na5elVt2o2j2///+JWTDTSlchfNAKZ3eJowsmMwKYADxmQ2GYY//OExBUsEiJ4AOaemGFanVVMChkwCbgwZLHLaoSWUnDiBhuowEdAGlWFAKqpUVG5yhI0Flabw4JTTaGyEqBDKMWsN31ATWGTb1UZC/tu/PMRmuWTcK9RseWNL3na5I7NZUiEl5kjbpb4pKy4z4MGLmRROWWdFIlUOL+LCjZ9qxrb9t/0rq1q1piNltgFuVbLI+n3LI///+vEsaEgGEig8jbrFOc//i9iqptA1MYEAyYI7ybbByYAA2TC6YclwYZ5//OExBcs60ZkAO4UvNmDgOg4LjAgGQwIQSJwcA0El/WhhdouqBsm4om3qeeTRGGstXa/jDjglMZiK2gCg7Cdy37bphWb0NPEzS5yzActfmzTK7eBi2WDK52BqtBUd7CNcv/3+dy07TkR2VjCMJZKXDQiAbMG5pOc580kLvv0c5EcaCq5xGNTHZP///vZP//////U1x8QmydwEIBgLiRS2MEZhnczXuSZF0V6mIsaJQVEfWmLQsoYGBIfgYiDKKsT//OExBYqq5pkAO5UvGHEctgIwDaYn0RAw6b0UBpbF6qXOmjEut4zwwBMflAQJAiz0QdGDhJcW3TQaTESeYlL6MZp8OwSweH5rrRS9DsY2XpU0pNWYKl9LlzK3vPfP3RnYs5dphMGBEFCFkUCyIgsTlSVTTS5Cfb82dHhMWMH5QlM/////Rf///9//6ujGsXlzc+raWUyVRyx529K1niCKqNSuSfFwzDjaxZmlLiAKzEYByBMToELU4n4fNG0TAb6//OExB4mCgZkAO5emPyww+FBedpmlP3Vv9ZE3v2JudDDIKlURBBaT0HVZhVZo0Ynuvss/LC9KXSliuZVOGSuFCqESBQcHFiUW8e+LUvrNMbrumsZzJR6pj+PlRv4VXsfXzA1r2xrf/3/93jPpZJo+v///3f/hEwWaeLOd4VMelDXusYUPsZI1XRcpgJZ4w2iMzaIgtaHBiDhJMLD7MezuMDxzBRLlrFSGYq17EoJFC9nDn6R2GnuPukgBVZs2Wn0//OExDgoQf5sAO4emBCN7MbUrByVNHZQTqTWirY5jK40xxw3Pfi9BIGAfUeOCNAT4/6whHjpOtV0iQ4l70dQ76xtk+96xik60aDp+xKZjvn6zv43//j5/+NfLe+xjvRf5Rzv///L/rrvMN9ZR/z/1HQGBD8IlIjWirQjBhTHwqYsA4kF2bgEsGcqKggMKh5A9yUBi/5ZkyodE4QF4tUnxAMv3IMpU6aFEIyvs9ae9+E07SZK0X5bE01ryhLTGhUg//OExEoxPDqEAOYO3KBGVDQU+Ikqc0UNQFeSd/WvLHl8ej6/GnyTF/Vil41B2/r3nbi9qVUNak5fs95hUf+ETmrwwE4XALAPHx4hmMPu9ld0Uwwts7jcx8/4+6GZn/////2Q0mehY+a59f///+qGNMMZXVDD+e7UOQ041x955lkNOOo84xTybNbuSVNwwhHWvDLrBUCMVohIKXDCrDMgUJuvN1nxLyKjWJVkdDhv4TrPuu1NX4tIq/Suk6q52tOm//OExDgm8+qYANvO3en6ab43ibAqQT6Fw7J5ZuwQp1ar04pS4lEqX8kBmVyscIr2BqHBdw6TQoWRoNzCyDIQDQocVKvWejNZ0Oc04sccff+5rMv/////9DFcpRpqVV6/////3Wl2s/7eOvMCOcm5z6Lu8qVVYAvgCDK2dKBSY4tRARO/1LPJUn8aIkZuQUzZXE521Kmwv3b1TSqN581Kp6cwwxpZuphnVnZE/kmlWnCP0sZLoR3EnT+XyeclSo48//OExE8mGgKYAN4emDyfqmOg7C/LSdZlfeGrWCC7s5PH7yR9ZlcmC8ODGWHbO/eUh3/3Xf3vOP/rcuQg33kW//9rRoRLPFirEM//9bYKfaowZawkYsIVxyxrPqFXxGwVxX4wM2H0JbFy4PliwZiInEKxZ3LuN1TVp9TtPZd5+L3cZTEYfp9Vo1Q38b1WMw4+7v9svrA6timi4FuBvCSn3Ejq10rlCo0PXKsUKOUByHs2ppRzK+CxObJeHBhTP4lL//OExGknMg6UANZemG4t3CtdOSgUbnO7bq31rVs/ePX5/xuXE2SP1ir//9S2BNxo0lLzCP/0LcgWvFvhQfBiZF3eeOpklDjczVdecpdIkDTVJkx8AfetSNjAKx5nHAFIOyyNINGOeCnH5ppbMuSy+X7+rGHHqbzpYZhtgC6L1HQI3LFQBtBfVgIKkoE6hbWrVM5Kln3Hq3KND4isYrk5P5AN8SOzQ2C6vzb4gx/PGe4Y4b2VPbu4zXriv+71z65z//OExH8n2g6UAN5emG3Nu8d/BhBsUNxB+r//SpYlBUigysLChD/+wafFmbdNCt593QNzMtPEpcnvyl6iQKbaqTD3n5Vm0qTFs4IbNT4QAlqDcnKytMvvdlL5JFskvdlLcYOYm/dvlh5w5JtOXzf5YZIZTFxGdiPGmMjJ9e/eq11coTxLTEmlYxEMjCEHJED4PF8cwLjp0zO4XTpDSH5yVXFxdeMFi9d/Xybfm5S8/tKO3tHs1jUaNPd///7dT40R//OExJIoCh6UANYYmJ9AbffVe0KlKLSz3PI11btz9clQ4mfFyJEMOrGYkz80sTlDfqx3rYQoadCizYGlUMp6mI6C7Ghv5upUfVDvHpmmsXHBZKYoJeiK1nCgtlBWCwyQYXq60JnVf9pyiVsOS0dEk4bP44JOhmH7Cd8rGTy1XBBs2fWbyGYoly0nl8/LTpKUn/VnP+m/a1ttHlvXCxdRFaAWb//+ppHVj711e01KobQJwhHFJYBAZ4+rFN6xjwwF//OExKQnQg6UAM5YmDT2QUEBQtSUUgNOcx6U0MAOCM4rW51poVZmwASeZxm4SFwBmyg0Bk9+7LWYlqRIwhvaw1NhQCPBxIo68zvKGXdaS+8u5OLjIyHybyytdUH/zSzwmAufRVShKAUaCuvp7J08tep2HRlRWsixKJMQ5D0RJ5acxLmQADQVMgEFQE1kr////zuo8pi1cofQJfXBUNIcKVtQZGRAGKukh3EQOCpfO3FI3KGwESzB4HAwKCwDcUwQ//OExLom+daQANaYmC03FQMDNU6pneYMBJwzEucuKYitMX1SsJAlvSqnxnntMdo2UlLmnRCHnYAgahwkNyXY5Ur1N/TwKs8unoTjWv6uSyqH6vuQpkYjjSLerY5cJyGe7M48ytz9n4eaOnoYBGYFAjXq9rjQCBoWNHX////2+PnsRPDQiFhY9iJ99uEzpWpqqzaSG1VFqr5Nd7wwGIjBgSMIBJgaGZgECmFReYlEpjMJpds9LjuMFgQaxx7JJwdi//OExNEoAdZwAOZYmHLWthcEz01B5qV5bwYQXRhmJz9jLYjDNEMFFtvMWL0TTHfCrbr5W4YSspNWP/WNLb5d/crl7OpuVxuln4+ztx5fK43G72pRSc/RikHPmMcLABAKmDcH4TnoYWRj9Su///////WmZep+3+idsxjFmKh55NjXQ91PPMYwwwwaNQwyvjhhyUd/+uqUzuPzSE4wEhOXeQMIJ4BAFADSnRMZdTIhAWCl8suedmpKPgc0bgqnDdhx//OExOQuE7J0AOZO3PB9FrmqEDQaf7YusgBgQeBXYzOUkMt3TBe18IGyxS/MCBVgh2C5+YhtTNnVfHur12YdmW538pQ+kUh6blcMtlLqF6E7xIDB1LflcFwczqXV7V6nEYCACtWQIYC4KHD0RoND8PhQOilInMFxzXdpx89zH//X/////8Xax9bkOjdVFNNRU2crNWWLY03kVMJU2atkOSubMFcdI6rS6SKuS4xEok5AaiAKLlxueY3aRLwhdqbq//OExN43hCJ8AN6Q3dWt4TbwFg+aZE4crcx7I2ICxxjKVvM8odQnmXggeLUbWHH8TIATV65f25KGbcy/9Og7Musdsw3Ly/7kSKrq9N0zy2t/9JYl8tx59SWUGdnWL7w3Cmm0NnCntzEt/LLVSzfs2WIJgcICYCio84mKiwCiqibjFUVFEZHYzf//////+h7+b03ZER3ZJjGrqifVvrZ9P5nS+Ij2HIRpFizSKSP91I/HhUZNXmTRC8uOGAEHiMBM//OExLMn+9aQANYK3G0M0NmDCtxpzBuAAQORE1wIjYp56IJdGcDRVMMKsicm1jnTy90F7Q9K33gNr6OACGaFS9p70PrFkNWX08ooY1Td59eUQ9Wvd5HmyuJJ9XigsCMMg9Iy40ICUccXMPHi7mm855dDDzUVrHKb////////29Ptto6mHupxFzMtrcCzSBYyA7NrPfT8rkWG3behJ0lBQxUGo6AIQaBNTZUhED4IGIyCMl+UMm/lKSZstRpmYcIb//OExMYncx6IAN5OvJtBl7c484S7X8fuJv5LrW86lPyrGWxMgXKupnT9tJ7DURpLXcXjehubuvw2khl+sbueRA41VEcR2B8JAPBEEYsIw3InZpxj6ztdKuOFxKF4kBMYNizvqk3ed/////////+iWPOqWZ/ApNQuhV8w03UQAwYJBuaUgab8ECDADMPQhMQCIM0i0MvosMEjGMJASR5MEwtMOwbMIg8MOAhSkMBAQAwOJyIi4CozakHMfJqr6KWc//OExNsk4xZwAO6OuIhDMNl7DIJSEXQkIvKbkxwsQ6nTuNxkRHUoXfLrpfyx326NBSMaVMVn/diETcNxBxJRLYnL6ZWxWhBdK/6mErkTKF2LscShdt33clmGXKemBYEIRDQweMK4tICMCu4NY8URYFMC8mFsnF70dW5///////nstKNzNUMPS7LZVtmHz3c6ZU886eeec0xDDz0aehmyujK6HuYp6Gc8nPdTiAkJ8DiijLtQaOAYRjU7ILTAAIBw//OExPo6rB5sAO4U3dgMKzAygNuu00wPQSJDB4wMaHI0ALwADWqGCwGIQKZDFpjIEu6o8YyA5OjMu0EhmWiEFAJFQFMcu+iWDimQJgJkGooISzKBEQ5goggsDvHA6bQLTAIMh1dVa7iM/UGVCoQzR5IehEXkbT5ZK5mStfZBFUFlBhAEh6CjwICglj77yBrcZdWJw85bs0cGTEFP612kBMQC5qPg+DeJzI0UNy8xWJsJEonDlflhvb9//8/Boyn9//OExMJBFB54AOZW3XX/X///XPDL/qYllXV7+H7lWu0rp1PZB9p5ZZr3MfGyZnjbt3PiWsirOxKidtcbywmOHe6DsE90Exuw5NW5zzgXduq5TzCM5n8oYpcJ8TAMD97jWilDxAWN5qMgdWa6YJCmwNGbBMWZ+DQgESmzLnYRTSZhUgYYtF0oem4iuJQFQWWp3JGqCr8RzTSUNTNW6yx+4doY9QxKNxPOYv0s9WjNytfo4rD0tiNI4MtdJsk/MXbN//OExHAstAKQANYK3RZ2cccLv73etfiNQiGAxQYHKHWcemjuLjiU/8q6HQt2Wv////0ZvXt6OhBZaLdPt//7uqVu1DTGNyke61GxGsNhAnLVPNIcTQDZw55OQEzQkFq9BhIbt5GgD7EIHYyGFiULjUBd0OZtZC2CNKVlN8W46lU/Q8gqisTwYIhpkqwlANoHKLSDkCbStmlZfRGxfdbpEjWgw4G/ez1OwnivUJ+l3Ohyno4vrYpfX9s61XoktzAM//OExHAnC/aMANvK3XURR1fcsP2T/6t0FneWj2////Z9KGLvzz7XT3/+///2dP7aHXK9RGm44cOS1aakf8CHx0xWBiQAh5jpgcxhhycFQ8yOGMhkwh0RHi6mkMxqfmC7zz0uCgTZ6tGzmQ87VvfjHod5VyZTAtyWAERb5qUsXywWz2POVPdqW9Z1sLX9uEnOMGpY0QgVUjMGrHOayKbPQbXY52OYbO44Dok43Egk9Uqi3m/+n06/////9F1Q444l//OExIYlyvZ8AN4OuOmj6Rv204iZPCUkElAy7BoNKkVQMBhwPg34GKmOYGCQVJ5lIjGwDiYRFRhcZmOgeY3SRh4TiABCNIxZYYtiyxMlnL+Q9AcRhmHrtmh5V3lamIjSbpYjzGljMZxKoxHa6Z62TU6PprP2g3vyy6WFRJJq09dtk7Sdr12vnp7Ma6tuem2rVzcXsu9CW/JC53O2u4v/9ZYq7tvZ25U7orWo8EgouIlLh0i7vg0RWIWAMMBgJMS2//OExKEmAc5QAOYYlODusxzE0AjCQATBsJDZsNwUJAGBJMUyAIlA+IL5MBgxT7Q5XA7oDJJWfOJNyap6ahNMbOYVKVaZMQlNAePNZvE1aJqLinteu5N63O5Yv/9TIUf8OP5Xn1tD6vPvPI/hd5l7+x2m5n9pkfmpHUubl3M/7TvpDiIRn+f7uaQ70y/LiIRb5Im7h79b3svjnkGgwBd6it+1XmEAliJi0BH93wYRAMPmCyCYcSgOBqmqqxkU5DQa//OExLwnW4ZIAOvGvYekaShQBnpgV/pbfyrWsfN4ikYQpka9US9McKDpq4QujCHPcy+ggSJJ3M/0/j8REJUm5+iAYGL9ECZTFmx0nM3VSzvP8iz24RYRCUG//bNMucnFLrsNqIM7f8z34W8nyHlSU7XvICAw4GBgYGLcyIvFuZOCTgRF5A69wyQQwkxQ5UWyZ6nmEiB0QMka5BmImaGWmUsx+3A6u0BasYWBB4oZZOae1AyUCf4KAwZt3779oSWe//OExNEm69pQAVwYASsbhI/phy5ad/5hrDiMGik8MgL0pfq+NwGUs1hzlPi7cXflQF3GM24Kb6YsxeXoFVX/wn4IikvM7y60DOM2zxyJXTE6STU0Z5rctrXFhHIkECt2FkTCsjC0UxCGQmhMAtJtpis5lawqCVEJrcja/P9n6nNVOUlAqJnznwAt57nUboziExZAE90ekTMXKfp/qlV9ZrmfN9z/OX5/nhzDB/bnsoU0nn3cB1H+ikSjdmdkTtQ9//OExOhLVDp4AZvAALmbOVDGa0RitmpKuXOWO///zPtPhhzmOGXMP+Qum+k3IKTdudpn6mpC8kbh+UyeVwDLpZTVakNcrZbv1t34lS5bx1vuNaqDVeLDBcENVMiIoRWvmBFZ1yGHFpgQClaMuJ0D4EFC6BgQMWUDSCZBVcQ4AqLL1f9kQ6wzAVxSsBZU6rX2yEqDMQvg2MvqEfXet4RNEA3mfiJNacF/5XahmTw5MbhMTldHYjdu3NyaVy+Gnoge//OExG0oyfKIAdvAACV3Uu538t//f3+OqXussOY5ztJnf7nz/w1zeubwq03b2dv7bw/6qP///jj5f/9DWoq73/2Lx9XBNR2FgRBBiQ6268CALNZZAc0StT5gwSYVNggFWCS2KoQM8ysyiAy+SPKtploWAoQKbuiYFDRloWA4MM7QSFgimOwuDhGWrAouMLDciD6aZgQEhgBHABRyePrtatIqb6WGpfN0sGs4fScpYk1hwHk3GH/d+vDXJ+vnrv6w//OExHwsWyaIAN8KvHEFPmsY4pKIiIOofFxoudb0YayjSo51OLOgeGIQa92+v///////VvXZkS6CCC6XEg5X9n/9alHH1nY0AIlfEuRkZ8AFE74apaV3jAGMSnzCAFWIwkCNMxTzRAUAVGwAAgEiPkUDLARNQtSZigHOAhcB5iQPNGJTcwUiGEgUTnSFhqTRmWKZMady/KmRKbz929SMGlEzHG2U1futKlhk3VA56nol3NpR2YzHqnMpVL3OsQhO//OExH0w6zKAAN7QvGGxYdChqHEkCjg8IIDACEB+PD8CaA6Q9pDo9W443UocrFg0EaCIqK///////////+OYv///+uk3XMJFxpR4EBo+n///2tzdbJMUrvABDOAJkgUa2dN0OMV0ipLMCIfFxFLtsigQoanPbpwpOlXHwqNG58h9iYUDREJiEMNTsAAVltn4a2YghmCnpQHzlKnKjA39JcxYTR3cpmDnHt7pXBdixyJP9B9H36VgTqQ5hK11SCpL//OExGwuIy6AAN7UvCGpe/lJdIi49JzTTSh9iM15ppIIhgWQ9BSDcWJhYESLQhLkp5AROPS5iMcQnFyYWiMfkAsqx562Rv////t6f//6nNLnipj///8ky5qVkkZpHbABMc0XrMTIqswN/YJZJI2MARgNsDhiH3jaWYSinzgClbLEjxx4Nzp1TixmmIoGdy/RtwG+UVMGHCI2lta83JV3f1ZXDr91IEtVcfY1TXGHp5Z1bmvqHGTAHRXvd1SDjX5f//OExGYoAiqAAN7emKuhZ932ZMdt+MXTjilyCGIuDpvH2zoeyaxmOy7gSQN59ojda0j+MyQIDVEkdf//aKzT//////9WDRug3cescTjPwdh0RZCYcSHTDKOMebmyYzVZJiFPtW4yA8OYnAi2X4jUZMdG6YRzZskSrwxEpNRjTVBUIAYkouYGAgIqa9Myt02HT3NyyX0X15XKrWXflcVxzr1qXv552LdzhKZyUUNFlcy2jvGGs1MP66xjWMa94rDM//OExHknSjaEAN7euPmVRtsW8rm4aeUXc24DzFsQptQrS4xDrWBJDzd3qJRP//ePDaq5Wm07DFoIycEgeHhUKNXJAhdZMnIpoZExGiiSX4iBDGSU59JNSBSgGLfAosNzAwMqoLAkdMpCBb9ARcBAMw0VHiMxAFSJgJQRo7xQrCZi8ZqVIxQ0lSk5fpp7uG+d1zHLvaiZSmkuWiILkiGEAkgzPLuUP7vp1z2i04uAoC2t9021D1aR0mt2Q/pvTbVS//OExI4nejqEAN7WuDJd8KOpWkvfF/+lYTNIexPC/KS+54yLOI2IhhuvBuC6VpIAM9ONstEhsBmNknhAH7xB0sxp81uk5t42ahTIkAHOXHWdm6XmBYmVVG2UoJDFBjHAEv0VltKqOLLoPkdHZr0N+xTUli39Feu0jT+JXKFRZR9GIBWPITSMwmpFLyjHY7W1tfwtZVZI+uxatqZPb9ZW1Xr7ssnOvFROK50ixzP/3FxEFXl1uz0pANkBhpuFmOvD//OExKMloiqIANaSmEyd1bxlohrw68IDMdKTUBEywfMJKzdbkCyxrJc2piBkYEemeFKdJjyoZslmOE5hQuYWemshxkwmLBKXaESFLKXWb2mppmXVsqGVdxmKHWNS6htGglGcKpAjTA8EwmDALC8m1WVdi6499b6qXjO7joIkBhEskrKXyFVtR2e+oQq/tbqHFrdwAvejr0oRkFjazv9Df//Ur1ofMY1HrcCPWbk8zB+AqHPCQBw58Gfgq5MGBVHj//OExL8oOiqEANbSmBmvByyYcCr4T8JgBrWBjwcAimSIzmKEBjoE9YEEDDiVAbPsrT5jNE7k9LqlzuqSrS4XabHC7SRacyeHkDHjCiY4BaePR7IVPzWy5+TMP9vSSWSRl/++Xv2MT4iSlinqJhAqLB+z/R4pL/JgN58iVXIEzjjievVV3E0ORlFgCmKGUdDqjtpcbMbowuFJ4gA4NMxwFIqUmGApnGBD7ZF/ndKii+ohYGl08GnBYsnFhp+DNoBI//OExNElCd6IANbMmAxZ0zmJembcKrLtPtZqVp6pv4lb3WhrV6dtXZfHHzjiZZWA/ObIWa63Zv4ZZ4fIr7XajB2whECCDQFweCMBUdDO9kl3xzqTUQ3//M3A1bmmuL+Oav+P/////+LhlhoGVF0eRNMdJVzrMxcRNRYw82R20Qo//71CxYWFlZSCh8KBR5esYYQCgWVc49UiA1eYyvnwapnCqBikwFENtWAqFhw60My8jlEgELjGukLFQ6FUDdYV//OExO8tk1Z4AN5QvONnLmhArSoml64E+QHHi0CjnbgqNS6rHpbS8x1fq81yZncpVTQ1D1Xufvq6oRC40oRtepFEOWh00eZR0alhUSB6BwtKABA6GxzoSFItEYSjzTnNdW/9TUdf/O////81lNYeMBcJRIcHfKjn4cOkv//yojekjpEqQkolCcaJcBB2ZYYmanDyYPiCiUcIt8YpgApaLDCZyk8HC8TBSYMhWAh1irKkyWOPW4TrXKt6XYj3WncZ//OExOssGuJgAN5OuDz08wpAymZVJ1l9YL7E003vrflifO64hYjbguOq/1pvy1tnf/zi+N/P1vW96kq+8TFKv8wWOPvFIlFCMA7jwCJvD8L310t/YL3v+2f/gxAzACBKiPI91bBMenh8Z73//6EDjP7n+t6dh5FG+2H7XgQQiymDkUxYxMnPD3lAxgZDDMxJGMqYj/jo+UIUtRTM1fTCFQ/eGMNNAMNptBwIZgzGTFRh6aZAfmDGBlJglgsoFBAc//OExO0qMdI8AV14ARBuVifabjhqNfplpEUDhgZkAiJicofyUv2YcSGAhZAAExCncRP4yGEgCplG3XlHa9aWJzIPlxDAQFMJmMDRMSEJStVdsUt4UmOddJV50AbNINcCV2mrz8TiUL52kjFt/H4kUUdRujS2nkACY2GkI6YaAImoavrQumpqHBpdUSDpdUr37+H77+v71yKZ9qGMUUxXv2dc7Yxyyzw/7OPf/n/+v5b7cs25frLGhzl7hsvl8FL9//OExPdSTDpUAZvYAFJrqa63BTBge//GloXBhM+41aK1m6xaQY/v//nPw5l3eH4dww/7UjpZfOXL1PDGNLNPxUj9G/9NT08+5DlSnWF/eNLS65LpbDMtpbsOuqyJutFVQ/EIMMHgwxtPAMTAcATDAdNTNozKDgwLGNwgcXZJoQXGTxYEDowGQwwNvqShAyQHFBr5YBxMJIEdkQBIHFhkhOwCgscwQnFrACLiCA8AkGAwyIPAMeLJF4XCsSwucfBB//OExGA0lDpoAdygAAnxZYyhMCfBwkUHWMMZgUiQUrk6cIEXjZak1bUlNUq9Nnd1tLpsxkdMk0jatzJI+icK5cSK5oev///q//////aip1McPzY877mepzUyMUUi0tAyLSaJ43UaIoqdTt////vUtzpuYMgqigZJIrNTZ0jEydUkAauRgCGFcIb0FQhApaAwDfDPgQJAQYBFporhmnxGYDBwQJjDhgBxmpSyhkEVhwVmWGHiq5r8CgR3RN5BooCd//OExEAz9ApkAOZO3MY9i7xEOeEQceXwipqKmmOgvUNYhejkOUDg2/hA4M504zJYV0p+lZS+eeMzOa/WHe///vv93v+a3z8P5zut5W+cFwlj6EEZCKEFYalhJHRIEw4oPyauzNv/9v//////3RZKVKjh4QHohMgJxEFpIdFDkCxQ8eDyI0qYfSfue6eum9Oath2TUgeQMPHz3402YWYrvXgYFiBzgDLBpGGJYIbcAAICBhEEGhEsbxBwoAYqYnIh//OExCMv/AZoAOaO3BAp919mAimoPceMyitYeghsFIigLP1gUaZZA0wAn5MOkCwIj1JPJrSQBU0rH5cczA55Yy/gdGi9NOtdpeTMs39XPv5a5+u/+PP3j+//ev///n8/eOEscepowphhacYfQeVlColDrjyf/+//////0b/6HWVURRxCZg4NIrEsRg04sFRo+Jixxp8uyueYo+9mfdKn96shyscapEktTtCUuMpoYdUA+xEA6tJlcKgrcGAQ4hyM//OExBYsK5poAOYKvcuUMljxW3ggA92LwOZCFEpZQFRgla/cC0U4a0dBg2SyiAwA9jzku6o6iayxNMzLhttYcEhydhj3IAWpS6fdrCJ08rzp7+8u7r9w3l3DDP/w57nfmIQXIRGERYXFCzq16VR3RUQ6RA9H3/5///////f/+9tZjCQi5jgCYVQUBxpHDpBEIBoxRS48VFxAcFDqhrH6fgW4/Xqf1zNNeihbqZuWegCqs0kxSM0ANZ1ICB6FsHym//OExBgtmt6AANMeuVVNi0CRRRPHTVtrzsGTS011cze2PSgGdiq8oEpuiJYysXL7MvTD6ePDGn3zOztRBBNDgb2eBH+IlPv022MicpXN1ehDeh6jnpDT6jmY2dgVks5+KBydIez3vApSPIrHlM7p73zt5WP4EeBrdP//e996vJWJfFKfFP//839H8f4EDOx0jnEQ9/KzB9xOHAAek+o9QfmP6Bn78zXve/wDqlGxmCmhcULrRRo+ku6rUvT/+tLI//OExBQsbB6cAGnS3Xam7VpObNKEjt3HiQaCpFDlYxVNeeNhMXHDyKI0IplSJRDE6cJ4nOu4YaUAYHyAnH6SQMLLae1lARsihKJEFAdE4rDaBJC1vQQmmuK0ZAqRn1YCQE1o3JRhwBx9AgV66MkXbFEPs/Gs04gMTbijI0aN9pMVVOQKZjfUhFpB2EjkUs9OROVbSjOc0jE0CqMjPqORlzaZvEYreuyToo7VQf//r/zrP+XqkC6+f/7xNzs4Dqe3//OExBUqjDacABBY3Isrl4u5p6x7RLDTYsskt8dd51qDqO3tb2liQnKnCUTl76g4QjX3VpaNjg1LSkTie+4URAXCIvP/esXjkQY4ikOQknh0IoKBKKDMOT4nnsj8Uip4/ls9uodbeWpjAtEgK0650rq1hxC00uT200Nxfz4j9tdLbiqp9sESSh8OxkT4jxS8Rj5UXDpSrHNkmJDA6OqPttwxKofyqSf537oz9Nf7pQ+5Cf+v7oHWfs8nTZ38787N//OExB0mJCqkAAiY3SW3uZWMOf19rae2nQdt/gYx6lNa7Yrq4FnLCWfqV1VtjhxW6f1sdE5f6QpvtOOuFUrmRYK8AGOFxmPpdKypUSHaqdeKXa0vtziX6mR06evtysTXgu5dbW+ek+l76uj3FjxwSoi+sYKcS60oqrGFzrHPt4nWrWeyKss20NqD/6//5///KzX//8ggNoT/f5/MzPW6a5026GDcmnR9M5n5v3yk/6J0wbVHRVZJp/cbXWK3WV50//OExDcmnDaoAAhY3HBs4VHqqB/L605umEhSLCKtPBIHkJB8auQR5Ok5LXCxBYLHaKCgPI/42nRRlI0WCRedVJDtcaTC28eIb5WHTU77rBUtzCwpqk11KVDMlCSYDxZvywZna6Jw7pHekvKWPo9kv/VhLK3//////////////////7XNf//////8rrX8a1K/tSrrXqwsLFCxzMHQeoxJvTTwczQLXSkisCwdADAuHlWSDU1g6AGABA1oaHpQsLWH//OExE8mvCKYAUJAASCoGzjgVCcOTaJHAqBc6yKjg5D0o4WBsDYGwfQdlHWIICoLThBBahxINQ9qDrgOhGJQWQ4aDUPa8OjZVRhxIqUogg1FRVBZhY0lTRocDf8BtwEY2jXH5rRCGXvARe63MUJwYRygxymtiUH6Q07Xubzwu1pJDrrpN2t/YzsV73NwKpeq9wUr/xm+Um7tycm7f08XgQvw69FbpM8ta7jhzHPWcmf5akLfVXaB+H6pcaXX561b//OExGdAVDo4AZnAAOTmU5S0tWkcx+qFYWH3XcTKkjPIhQSeGoenKjSb1SUVa1PlX3aq53JfDlLWjeFPKqaXxKMK3ZwK1puL8ujKXjXThO36KrM/RTtPnatZU01ftZVKSXX7WeElu0WFWipsqbOVQ7hblMq7Hb1XHtWHeWtXr8k5ytYm6uUxVz1n21T9dy5dsVrv3NfKca/403YxVjN35F+NDSfLaWaqjD8CoCYOUqkMQM9LHMjDUBZhY4cozQNZ//OExBguAoqYAZt4AEi21DhaRsLE6QuqtKiifPA53aGIpDoTZEY4b1Ju7Lt3Cy4Tmmu35vnymC9qyGxtSoVVdYZYRTqtrcMvTTXSMZGOA3EtcmbEsW865c4ud1dP4mWccDheZ9jNT8LayRlLfF9uV/uGxuesRMa+Ka3qO+VzfGpita+eXF3udf///3vEMnMHxKKnQoLveS9gzKOlDjgyH1OhISsPKwrEwi5qlWhnzXaWMkAmDt4HCRc5bZgZMd+5//OExBMn6jaQAduQAJkwm2kxaIQBOC7WhpG5PC79VuLHzWtEW8+yzEaadjEdnOkEKzzUWkPCQUyHMD+gXYkCYJAxEFCDugsZwZhM4aBhhAAkSbH2I4FvdBx+IefQRMSMKjsiUicZNAxJw0MZ4nz6B4sFM/pJmFCmXEEDxy6v1u1zBARjv//0Hw655L/1/jQE9Clrcm2joYxwtY9lVgIUOmiaK2oe2Vdn4qK6gSM1nmGkkv3GI4LgnO1KrCJHZ7uB//OExCYmUjKQANYeuCfw3yAYH7v4k1+/z6zlSDv0qUt+pagMRZQAUz+SiUI6MElVy6JzDY2rQkYdEVpeJ4x5Xk75DlibOUjXUCl2SBr2Urk8gYYom3z5PPcQMwWCPnL2Je1IUR/vX/38azGwTIN//+q56Ty6f7vxiXLzCPd0HIYV/mptYA5jVsTTpcncaP4AxEE5R96Bfs1cl8BKTREr1K8Ehg5P+oaZv3PlWGWD38Oag/HDkdSnRzysUqFpdCTd//OExD8mgfqUANYwmEZTMlIR8qSXaYDAV7t1hrsPZF4LcFXLN4x2lisHU+FmrLY3lnGqOWV8K1vGnsdiV+ITn/jn38u71e/7V2/3Kmq2xwdG4qHT4b///usrnK0f/4rKsKKdVRt5Yiv6LV1jRsa0txcUVbiOow2EK8kW3qOW4SgaQ/EbTJE2Hk7Q2BgKF4Y7XyGAPx3u2GJr3LN5fJiBF+6k6hiYKikHliCgUhYjC7SxcrFLQU6dL/edPkOzBhkh//OExFgmogqQAN5emAbpxo/Uh3EzOt/O1tV9bw+Y93jXbU5EjS7YZ8xKzbvmsHVMbzJHvfL68ChcKmix93//9aco9r2OTQj/q8FnUXPa/qXG/QvUMA5imSEGa/oLJAEwsSOXjxYrl8cZiFEp5gysVHVpAqBFgFipGlgg4UzTDPcqXbI8qjKgaKQzl0u2IwgwoTVlz6u64TdaPOrEnKEtW/a1az85bvoySTiL8Ko4AfLMDtCUvvU6MYrzCtdXOayt//OExHAmGgqMAN6YmCa05qlsdS2sirRfSL61fc7rPflVoLOWErv//V/+fPf/rrSsi5ehGz1KgONQOuwHAsz0DTsqDMVAYDBIKCUxsHjOc+MgCIFDaqmSYgiYjOocTCoQ8ZblA1/atLEy3IIE48uxxdzNozc2nsWaQWqq3FyhkMZMYgfGe8tQzDlNniHIAolKbNVhnb4uVdm+uKNMXa8zS0z9tnP352rVLNWrmslUnJRFLNPW81syy7gFWtws//////OExIomCe50AOaYmP+wNOXJVsaqkUjtH//01WVkQBggEDBwLjGIujR7wjrMNDBkGwAAwFAkwbC0zZSsohUMA52UhzAcHzC0WEoJY8kAQw3tm5LDMT2BnE+taZHmjutA1OmSaZqMeKSJI0LiY+xwmRwmCKB+pOki6R885s6CJdmSi8L0kUEjdJq1qZFtOpbV1KXulXZSRq9amvfu//1///f///rqXrc8iCIKkuhboxZI0dMiQX0//2ypU9GKFAMa//OExKQoGvJgAV2IADdkCgSZZX5UGpiWnmYiIZqE5NZQuKDXhiM8rNSQ0dRwKDQsMbBwwGEVLZTSDQ5dtNcfNXgYAtFN38AEYWYTURzFFlLmrvNyP/Fs37rsDSLha4WBq3jjBebWu5fqF0mcUpLxZBADKZ17FH5r/7zfumseB2sM4fhh6LoFEQ4PGnyrbh///97r13uuioiorAXfLbqxgYhSqCl2tsmwWdXw7aCHf//////+nRAkYjEUdty4u1x///OExLZH7DpsAZzIACKP2tVnL/qQRvflqbztlfq3e///////X///bz5rDX/ynn8KTDvxuQ3HWlToy2BdXoMsZ3ru///7////////////+tdqYS+3nY32pKJZTxuXxiUSyWxqmvv12VYV4rLJ59pmq5OHakYlT92SuVxYEQBhpiqwIvYw0IATIxs94bWw30thxNWAndsSV+5bZ5qIDKLqLGZ81RQTJ5eoorMDFJzpuTCkiamgw4xB5lELeA5wUsoE//OExEknCwKQAdtoANHAaHSmk54fy8fHoSQwgkoviUmpQSQSpqW7U1IboIovTOGhelNMl1JIWvPM6DNX9NNOtWpSqv////9Opr9Ba0Ey+ZBEeKuL7WaT+lxQ4a//61G1KoNfmgZ6DQA7kELXDICtcwp0P60zUBNmU3PLrpuwicACqmZTQ36EtjGs0OM/egWremazWs2ucvefAJH1oewIAChKpjPzwnOUGwDxqUnwT1L2zCLZpjrtrLbVPivVOJmp//OExF8lev6MANsWuIkozLTznoPbLT612+Kv+OkuIma4v/////////7mUWxOxrva9k0xdmx+v57HoArFbPT40gEkHjd4dxl7jEQINngwUAIiDBiEAmVvYZFHAcPl2qLGCAUv6kp24GChUlFPVbFC9/aiMjhVL8fYjaN3iBLrF7ya+Y9LazJG1M/aVacqpUT22dSsytZWW6HpRDmaXea/XzX/4rX/+0v1KFGAQTnaFAQEwUgEGdW+hn/////8iKp0//OExHwmzDp8AOPE3DiS8y5nLXtR//////+/Vn92uqGupigyhxDMZQokoUSqbi9SwYUFhlrhGWxIjeYAERiMKnv7WZIBxMB19GECMsWy1NP4DDowsAX0zMMUqmKHeHM0zj4CGhVHnEIOcWtsbm4y5TjirrvFYn1bEV7OlEcqz/Ps4fh5F/08lYWZTmWiEKboG9X371//xv//7L+jLVjUlsZHbmT/////6KV4QrvVXJVCNZ////////TrRl/+krCQ//OExJMlpBJwAOPE3YwtNGqZuQapkarXYkJEKhAADFRmOmqoEhswMDDDZ5NfkkeB7WxQMGQC8NAFiaCAwMCBosvelE8vmy594XFUaqaUqBteoHTUDZPCMKsueyiry+GJ+ghiTS64/jsQTYnvp8Meb1jzCx3LWqGSRycu9x1+fd/r/7rn/77s+v2lMRHb/+ya1P////dEdNnVCMZECohG////////6202Rq1kpnODYrXG24ABKJn0fIVt6giMQqca//OExK8nnBp4AOYE3RejSFwSYhjZgMNL/UCMPHBAfRMGMlGNIbaA1gRjRkE30SXaICyl0FoJzFmWwtCU+VBqOUXlEAL1iuODq7swG407qkh6lympTD2X6y/8M2axrDhERhUNwDweEzzix7xxiaPNQYEs9rr+r6GJt/9Vuyyo0MZv/////R7mnEgK4+s/BAMjXt0f/+8zCh5QGWEGB1W5K2yFR3NUCGGsQMBLTizBJFaBUMTDpYFCasDEzXwFmUUL//OExMMoEt6AAOaOuHZrpZDDjJHmGwqF7WC5ZnMKDiF225GWEgQtNZZMYcFAZ6jlG2JXNJY3Dtj7jGbtjtWpfDo+3E9Z16Zle70tV6EQ3VtW3LLmz7NSJsV7frNmDPTEOyBs3VuEig+wUGHRKxz///lRKEGDDbA+XiEANaU/HdvcuRYStPMpt0sBCI4iVIZy+hKb0nHsfwBAgjHBMHWxCIbgkHStOoCApGeIQsILCCbKyNbX4FBEzJWUxdyeN1En//OExNUnAZ6AAN7YlBl6OAZO4LwftnC+62VIwyB866y7e8pNb7Nr7patM8FrHkdv/9Db/cbkNbCMwfqzHDzSYhEodKjY0eG44ExQ4oaiPac92PaaYiX+8y8ipVncornL+//////nqymoqmvsrHXHhoPGiUXHSFjUHJBKFDYgct7B6TzB8V9amalQVbT1AcwMBMFATF60HNLwhUjMvbDumVNZXAXQQ6Vm14gokdhLCJIMuToG0UJavI7zlXqxgadB//OExOwsq0J8AObOvFdflYJBY6kUeOtDjIKDQq1FFfyPNubpQr6VXVr426s9uhYlU5G7Nr43hqmiiluplnbwQe+kOtnHwsAPjIsDeAQAuGJw1BaHSUwhY3dD3OJR+WLnHtRP/+vztH//53////2N84w0lPQwmdLG0VjGQ0fsij5HvWItmIuWnGvs5FV2Bw6gs/5hh9mHyQZaK5l42mHpCYvKBjcMJXKViwQC4AABIM5CLAcM152VYV5F3l+InKXR//OExOwsw3J4AN6UvDSXFjWlLyKhSoGncaXTU0w2Rm1N145ZTTU+/WMfbvAj/RyAHW/5LYpq1Wm7UeqERqnfVrUPy5/qFxKGxCw9JDjkc7+xQeEITBaFI+Iifc08xiMoLB7EZMSmHor3RTn////////6OY9TWnZz1nHEXQXQ2+t/FoAh5gJg2aiZVe0wqCzAb5M2gAxCLTBgSMnVMyCYDOxICAqIypeswAAw4ky/sFKwYIT4cCAW/CwI0qMORrQB//OExOwrcz5wAOZUvQMFmDc2nK2DKYaBiIkZ4wAkDuNjQThweHmdqZuE4jM1TsepXAVPAUsJ+a5CoYRwyDXRAuBONwFQ5YbnEXQayTOxCQERtUBPDwlLedcCjxn3/TXx/8PpW0yfEnpC1SuffNYU9TKg0O4CNO///5CObLvmhzkP6lf/saiy15O0wPDzJgHLnKiDlCUH8YBYqATC09MMB8aHKpzAo4ctYrenQtM7oi5416swIYJMcUVDbcDEMIQ3//OExPEtwepoAOaemBIRpoy4YpjboGNfhx18GfDQiBuvghTLrjc1Kofp2kTFPDDLECc3OKfZLlWdKzrNsMVx+exxjLgw7choKEG0WRggA4RZGLRETBdidYgshmvtVXNcgLGoPCRZ/////////6/tOYxkLD9yqIXGcaFjAfQG/JST1sM+w2KsAgnBBpcH39H4CTLMM3zRQGKFuDD5hPRVRB0Q6QkJ4NcEmFuTbiAqNmE2rO2MBR6NSs4BqgafJCFU//OExO0u+zJ0AOaUvDjYpKhqAMOiUCSt0gdWSN9ZlbuTKilqjpyEDAtLQJXS2GnaTGgbcph2z+538t93+98+5l/7//7r90sZtbsWvqXmmbb/XVGcS50f//////////tNI5cxzsYXLIhf////99HecqnCOzhnCup1kwTBMCjQogDYMFCgQDBMMzOd3xIXUnzDZNDsRYTFwBA4PjAQhjFICUdAMDRgaYgsfoQCQ6AhrSICmJkJiUsb5+mhhqSJhkeC//OExOQmxA54AN6E3LwRRLhEAQZcDvpiY6OmPANokATEQhm0iFRELBacMcLhCQE+sxIoz9l639luN3mW7eFr9d5ncz/mPN1O/qtnrGzjzuGVrCr6gQcOGma5qTVs7nFRgeBjPTa//////p/T/puimEViQGqKiQeGigKJCYFFTaOv/+2n+ljILBJxUOhweMIBlKhiirJj4KAULiMYS5gYFjcCARAQbmeURAomDBQCTCFqDotWwEewQKpgiEAONARg//OExPw0LBZYAO7K3HGA4BGCQWaGBTI19GiSYJEx6TEagOOiww8GhoBCgJMsANJCSF1iYYs1ZGYaFpcd7lHQgA43VMVjS6XRrG99iz2/l23nZ79J+dXl/965jr8OaywMzMlipd1V1ZRM6O8j3XvtmNvT+q+///////7Nq1tyozkcKJAP//wMYZDQ8JOgGmpqxdcwIBYypoQ6mHoLhKYMAIZGtYdAE4YDDkZEkEM+8NCqRC6YBgAYEgyrpH4AEJqG//OExN4rEypUAO8EvEmxjJgoKZsonNOJq46JAxjCmaaoqa+Fx4ycrBwHIkjhCImCATZ2UEQu7EXmaaxVh2r9rLG/nbrY3KPtbK/yre/V7DP8v3Y3zGk25o9WNnmnIi1OqQclzkI8ltKfI1ehW/369H0N/r6/t1o0lsjw4Zf9f/v3LogDQdMOQXMTowPbBaMNgGMTQrMgpOPzD6M+waMFAVM56CNzVTLA+GEQLioMmBYQjIIGKQmHAQy8gpAYaFRi//OExOQpcyJMAO7EvLK5yEimayyFQaIBeZeKQKJC9C1JjAXAoQMzXKYDD5gsBs6JACNACOOI/r80t/K9f1X7uxYnda3LMP1d3apLFj69vPWFq7qvxymMdjVK1GZCMtDteiiaq268/I69H1o9/v/////TlOtFRlD0CI47BGKE/9nF7S3tKV1FQwRwIhEBSYEJHhjeChigGBgHAMmCOKoZpZV5hFAxGB6AYYIYLxjSuOGRYA6YBIAZgCgLmD8IsYVw//OExPEtqyZMAO8EvEIYTYRIGBGMAoXcwRQJTJYhMRkU0BFziWEDHaZAKhjAcGrCmZTHBmgVCwLMSG0yaCRoAF9TBJ0MpBoMEjcgaFDHgcTlTzKoHMSi8xKITDoTBQLUSS4SrbdOeH4beCPxOJyq1UprGNeX/ZprFSp3Oj5dznLFTGzrme7G5ixYvUh0FzKkJmMZBM50VNHlIJh45EM3//RLIxCEOel6nVLE0+dEIxGWlqOdheLMgfDwDqCB86Oy//OExO0/o+pQAPcK3Pybd9f/6ernMgwLHjiETrielVoIlp4mGzEf5KIGPYKBhhUMmwKwaCCZhkBGFhOZrqhnBTmNQgDAGYDPR7FEHRSqYABhhgQm3s+dVTZlMTgoFjC9P5T43qizGoQMGnA7XQj30qM8AQME5h8pAkbgY2McEgUYUAphcEt8DQMTAcCARAewkwECxABh0EIShwBkQUGhWkUmKWAADQigsGABsJdtZBgcAsmgSYaQ6lyll8MYUtv7//OExKFAw1ZkAOcWvHL6TbzdBUvSlxIJ8uKCQ0kG4/joXHwigRj+mosUFh99xT97mc5WXoE83e9//x/P//xHzH01isV3f/11xb7nm29PP1TmxTCXBwwHw+aPHeXkhEhB3vP6AAUYrrfZ+WKuviOPjmcsL7nDFQsJOyloYfFHcMA8OPWQhhpNQXQWomgZtdncwwYaBYkExCEzOyvB0/MHh0wGCzFKYNQJQx2K1jAIqmES2bsIxZgwIAGBBQTkIAcV//OExFEzs058AN8OvIu8QsBi4jmRJ4FTtfn2VNjnqkEuM/LX2to2qEuG98iZACgciIpg1hrMcllmFym/hS3p+pMCcoPlzi7UHi5YfJiETD4PywBBQfCMAIKlA4PKu6KUMPVXq581ijjceqrzKf/////tecYf3M7amEjHjY8aDw3Gp6FxgcUd///8UeZUKnRi/gCYQCzxE4yPlWVtn5JQxSTQNLjhgAZE/n8KBhAcgYIwMzSLOilDPhQtcYORGozB//OExDUr0hqIANbemKyvmSgbCQCPGkrRiwqoilWhiYwKg4jU3l8VRNafGLrotw/CMvq/ksyzeHmhl1KsESc6SLsK+JIhCcalwn2eHBiP4cfEZ5qatta3bO6wYD+8ZXQVZAVy6bYUssW9d1rvfp9f6xi2Z8voUzP///ewETZ/8TBURGGf///61e3az0iO0V1mIPwNBTHayqaay6UFmKnmtFyEyRLsM3nDY3cHIb/I8gUIOQSQUplgBAoIYiPG5mhk//OExDgsaiaEANbemADPCn4YuYGwgRo5CnFBAEETAAx056swFn8QsYMCbd/JZMPq8sPs7ePkmqjV68EmDeFsLgXhVKVWP3N7JHpl7SJnvs6zi1N41qlrwVczPlpkVVqtsvzPr5139nkTV/TOqalaoj9khxKhwQGf/9g1QGAoGWZaz63BqKi6qVZpnoqjG6pq6l9vqjCdYkuNA0oUsMiYx4sbeDwuLGFyQdTK5bcGARi60Ct5eKjTOzChcy4tahce//OExDktO2KAAN7QvJMAHTAVIOe0gWrScKhQ8Bv5Ga74trU5ykpbOu0riy6xQTrAZDDjoytJZW2alsim4ct1t4Vf2hUNcuNRc13VDzpMMD1xHdNxQgsPwbgvF5MDAGB7C8fUKbNFCGH18/MOo+f///////n/iKfmESq6r/+f/6/47q2qxzrN2xdsD+cRnGVw4loEkNaAKBjCUQ1cwFhJ0YSCRADETP4OhpGY3yDDU9pxQuQSVHZqStgTCkb+1Gtl//OExDcmYvJ4AN4UuKJv4EVteAwibPIYe5KZbLaa/S4U1bO5PdxqNaXdLcbMSYcASI0iqW6HPnHPTrQ7qPTjigtThWJlIgbgxBpKCKEUTj11QfOccccb/+b9////0R+rTmnHHXKA7CfTyqHf//6I9SXIT2XEYQ4AFMQMDDOKsEwADVPyu5rMwmk20Rhtz9DEtFai3Kt9ArTNYTePSfIYKcJ0XcFSeC1iVWsrNrW8V9XmIdYFIua4tBDr0ERrZHO8//OExFAnawJIAMvKucidVrqqSMqMMOxzh0YAxsYYgiKmMqoqObZm9fV0VTP3b3lsjalR0o9mIWziLGKgsw0rYKhK/+v1+ydl5cu+z6yWoCxQWoOjaYG3dQN42hoF2qwSGUwqtsnwyPX9qw8yzqyJzf7P0vBzb7HHkUzbb9PnCzV/sOXYmaddfixg4ODCJv2FisqEwewJjvAOAHFQgID5hUOAgXlQRC+I/0WGEFA0Gi71IwwGg0G4N3LF7cx6B8Rh//OExGUszC5cAHsQ3URKM30p40ly3ivngUMQu9JHppPoiaVT28Je/fx3Ei59+iCg09EjSpFz+LMmXn0tKe6m0MgUm/hTC3saDc4ODULPIUs8YKO7t8vYvdQYk9VPsNNtp/LkcAEubZLGc0EAfbC7VJcSZFmkoFjEro387F3Evzt6pLp2tS4Xa0rqP/UuS6W0tJSQ/Z3ljcx3SQxLqta7L5qGdQxAFeNOs2Oca40J9rTS2vKzIqqIoTAURORJ1xKz//OExGRBNDpsAH4e3J6WMujcWht0h6sU6hfo82CaoUYKUsyKwy8OTArUq9P86FUrHSSWXb3URzo4afwZlwjHOMxx51ImmKmroYrH6XWXzPR8sYV8TDezv7wUehkVdK1zj0g9npeeWdtZ4zhp2nHU6tao8WJrsmlBWd43ZiOeXjzM8+4OIkt42VfZR1hMcKBLFrIp7tFqRWZCU83vnrFJud6+hTTqjeMM9Je3sblhgmQS0KjFho2MkZQvEMYz9+pX//OExBIoi+KEAMIM3Ridz321W5mpQ0c50aFx3S10rNXzLdN1EPwzzVnGyUqQrFMKj6s2gsBwCIKAbAtD8YEppIsNFSzLwJAzKTDVfJc1+clRUHFbX0kxKqdnJTyqSOh3bl7JWvhibV3mUH2e26XmbG+mrxbXKvXRlmrcN/Zv+1bONtNWt33/Pnl5zP+uHf032md8632mJBvvgxZmxiqCZOuUGuJuZoNC5cEyKKPABjRA9L5JsGAzPXyYQlZNrlKj//OExCIl8vp8ANrWuG3FcS4xNriPeaI73+ghGyv+qQQNbJZ8jsA+DsJh4MZaSzdZpPXPvHwsLKBwbvNDY2WStQ7cN2ZAFrlpp11/VdzUt7dTVTer7v1KUtZlRc///F///7455jrb3vu91OhNRrkmatP/Ih7rDBMDljIUOh2sCiIOmaJsM02MVoxZ3gROgy+eNaBgIDLqApaLAM5E0o211ciNnkNXT4bEOjeBF1701mHbeIG/SBv3j1pAfViMCtfR//OExD0nY/J8ANvO3dTluO5RKQGGWGdnVB/N6HqBMQDQR8VDCRF1WVRNO/fwo0ePGliRBqLgGmFh4xlr56bfQ0mWRjjtY8OTzv///30n31RWshomMIHWnUdP//+Y1VUw5EP/Rp52y9dzqsWQzdqC4y1IyICyhdqgY+SQ8SCsBBgLMLA6G7iqZMFJb8ASfmkjvwzyz8P/81/rG/n18w9eW3w1Vzdmi70ZCeQwWcBaE2URupJm2rldCohbLALeJsS6//OExFIne+J4AOPO3Awpazv66rW0a+4OAqTFhYcfoeynPtY/znnIUQ1GbMPT///2a5z9TdVXPRTTyRrGmOynaL2/+i2LGKVORD/+e5Le7oelFHRwdCzfuE2hvNgCvAiLS8LiTQ2AhWz8qARozZ6our3DwbRaC12TXUN817f1XLvoulI6jXIGiGI4uHYNAuaUibniYbLDsIUeCTCxu5ezjU7mTcnkg1XIANGAmEslDsMXGk2wnpl9G54kHNCr5e97//OExGcnKvKEANIWuM0OVZoc3yyYZ2xjN73ve///99vu7ZL7/m/mXsYpGzQBMHxAo4aB8d8+fGF/stOKS1ZtiFeowoMB0TE8Jw/QiT+RumWj763Wus55v/7/4flz/53XP3+eeFzWuU9PT4UcPxyAZqtdtwx2znN0D+xT5EudbyVC1AsZLluascQeV+H7aQ6c/AM+xpWkmCWvZcYALMW8uEAsJYOxXNJLH4C2UBLEwQ9FlRQkgsY4zTxGSkMnaFl8//OExH09TDKIAH4e3WFBn4kkoi1Fg504yp9QIzFIKFubxT1yyYf6jySRns+p3DNVIiDocD8ZG1WT5YJn793WzqHFV8ZkfTPk5lWXrGfxJ4bA2RY+H0DVppbx39sPKTPI/urGTdb1+n+KYgN7JEy1QZ1ZirJX+kSlIdJn+Kv4mdfGn9od41VBSI2ht0Yv///V84x953j4pncvzv0zLn5YHA/Fa6oxN07ysRuZHivhuV9FvL0hYrwgJkIeuX6ztTmm//OExDozzBKcABPe3ccVbTA9IMEXp3BDUEeCjFxD8DlE1M8OI8jRHEuVESoyoLYX1dO2MtsdmYI+7NrKpplNDcHkOFGq5MLy7DJA0wQ8/2s1Q8yXz9Yt1e5NWsRsYZ4TJpo01bfwoUTDu9aMcaHEr74eu2eW1WW+IF9Odo722Idq/13vcnt9z23eNG+4DyL5p74rJNLTFb03LiBnMaLWp/qLKr/wf/+v2///fvhnMxzaSpqatpiqTlbpFA8fKDDP//OExB0nFCaoAArW3IfKTHACoAw7TE1Qdq2uPx4EAbwxAQD8QR5E8TnCRA4dJihq6zVctTg0QVOGh1dFrU2JHI02PY1tXHfPFXvZ81DnzNMYaxLJfKFyePUx0H09fWorlznypRXFtfxSyjEs+gdbLYdanUTfxXMtbszjbma5uHVENtB1O76pzqjpenKq/59f/z////n/Hzv/51fGMRK3Vks248fD6A/z4zyLhrvHVTmcqvbF0zktcIfmhQ8UdHCc//OExDMke7asAAvQ3cVKHAvUo0I9ENIuIwDgoLLTlCweLl1YyLPQiFcaREQx1W6j7sdpdykSb/PC8xfHtDmq9RfC890UtXnCvZkWNkXSXnRj5iLuae754/5lFuL7S+erQfSG6ie6f9fpZNct/4Alfgov//8Y+t/f/+/re8f5lxPM9zfxNS7tK5tqtc3JkhISh8xkIUdA5y6mReFVfjwYjns+CgXIHJImanGxeaVEzbUpOD1SldB0P+vmH2Bp9n/N//OExFQng+aoAAvM3Umi9XK9NmCXu3fvrRv8Z387FrdohanN2lq59UFHEUs6PYDkelngpV9NCSoTMa9hAzDEo8Q2zTl13urao5Wd7t3veggWLloPedPUj/KX8v////n/X+9/Gr/5mk1RqiSbtvbFNK/V8dpiuMAnBooUHEJySw0TDRR3NqzK8dxtRVyjXxwlheq80ki8VMCQQ4a5ySSq0jGLK0l0KKSt3rEqb878y8afHrsbDfG+N3+tNVSzF/4o//OExGknvBqoAAvM3b1Pfc0xBFA5SQEkiWyDv3MIFXb02F2VRWnTESn2r68Ps43bXfK/pqe+3p1yuXY3Pnx2T9Hl9ss86+EPAxi5ibl7GGEjIWhZ+Iy+PelKav83//3fP+We+oE0ebC6nkffMus2bXsFOP6KKMuIqWElRjOsmV7xW1xr7b1/CjdiTyuS7OnUJi11kBIgomHlc51GyoUm/XJV/qrtVWNfwpZ/+vDpa1VVdqrcP1Ji4xn8mylL+qxo//OExH0lA8qgAHvG3ATHD+kGP4x/7FPh9mffCkrtQWBqTqPKjrWI22M2AXAxc2JTUsqfgYGKhRhIQYWKGPoRtQIyeYVkAIE8O46zxLCWxOOGM7v1V5mE6/3KZwh+EZXc16UMCZ24Lk4RKQ0tLyI37V+VQ5STOSZC1YYfa8/0zLca30tnHu8dZc//z53+xi1Gbcpr/l3r90VncrkIUv/yt/////////Zkey6/vXXIkh2dP69r1un10vZEVHIqOHMz//OExJwnJAKAAN4E3aqIxiPjgAZrXA4ocddBhK2OU4YRF3XvGTsnYLj4pbGMII0NvHDpVDAJqpe1NQ84hwIRtoKAS6JAChhhzfLSYPdxuQ1I9dynrH/lA391GoR8tghezLNbpYY5/7w5+OHf/djn42bf/ult7+o/dvuc9DPoZyEMUA2R//903///////T1vPIfrU6I+cxyElS3/71RfX/+Y12KIkCQiEBA4+lTgt2GAWZPcoCHqfgVAZioOHkV0H//OExLInNBJ8AN6K3QGSWTGMDDM4MBHOfQhApkhPCwGd2UgkTONGS9wQBFAWYegDw9T1pSpNqFytguam/eUDf+4Ls//vzv/oJPlW69KxZy1bcdtMbdSK9/VS/+vpv/W8O/n2/q3acXWPnFlZVL7HILiQMR3//S3//////6ZGdVdXQaKDDMzVVXlWWzMN0/6////+iLOJol3Plj8S1BMbIDREP28QnjjOPQCwxMFQKEDBgNNSvs3sKy/zkrmNrksv//OExMgoPAJ0AObK3cUYGAYNZTuRJdrfJPGKk0an3MgwaCYrqosO8vPr0F/9R/9/XjXf/cuy+u/8O75HFH2I6fRWylsary7v/qW4/nL7PM5XW79yUsmjXYxVMUqhpplnSeNg2A4Ry5ptv////////6bsbYfmjyC4449rGsr0mzUc83//ov///56GmuWSlTFH8EYNmuogILOyiGMGEcVEKYCgcFAHMHyuOPJhAwMwMFgIMHCgMxRcAgBo6A4ETP1e//OExNopu/5wAObO3DQIEVtEIgMsIox8JXbeovoKhdPmOUiikq5MOw4uOE3WnvvTD82ateX02HbcJp6GpG3Gee3BYcAoryu+khqYTE3Ga9PTXLd2miMt5Zn2HL+i683Sm8zhYRNQxzVPsVE4wAoCoOBY5hAhY5/93o3/////889zCyIyqae5xorcxB6df7Up/9k+073/ZqJ5dyy8XppsaexgADJnwGRbFTIIBAweXgBGSkmKhAZrNUczjQYdgADg//OExOYws/ZkAO8O3SDCQdjIoUxoIi4JVBAwbKk2gfC4MYMdHGXgDHxUBAoAYwGA4CQuXwWUWi6sXduBpbXrz8tx+UVaXdBRfT2KfGGIcpYhK6Ndb8MOa9Yp5bZsZbxx7j/73jjhvK1a5YrQ1nLqGhKKrb9ltAZxEDHV2//ff//////WxuiIR2KdLE35P///q///+jMQTLK1ygVnxgIORoQEKlDjAQWDQAXRCBxAA6jpwiuRm+MitRiUUBKKIKCB//OExNYrm+5wAO7K3SdMKxLMiRaMMQGTrMBKzgU4yEyMXPDPWsE7RkA2g4XgLPNqnoDgB3GruGuuGItG3/jktjDsTctljsRGUvI0t0nHslzFV0fXBi9BPzudNjzePe5c73vP5j3mGu8rzdJ85Z6BTHaq3VVSywCFiCpq//23///3//t7V784qaYePKPKein3f+hHb+nv/1p5zirKf1/kVTVSU64NS+KoubnymeAbXDEzUxPENRAjBg0qGxvoSnUF//OExNotW+psAO7K3EQM9kTbVwIOgoFGeABo5IrEioZ1DGlF5nAiZcbG6DRwY5WMJIQUfSpnqKbexey6ceiTWHPg2nkrL2avoCAQqArEpGyhmKgsZlU3Lcsdd7+Xc97z/+Z95vv9/eXbPZTKJHbkmKDUJVE2uuokxRbHCW//r///0Mde/NoGM3/5VUuqDghykUoR1ddyVo9G//6p01JcSaM6ktLV9uANAZkCfmDAeXIMPqUyGgC64JD5kWGmfhSD//OExNcu2+psAN7E3IKGCyyaFfBk8GKyGRkca2cA4DMADSWGMlAAcDGOKhnC6ZUCgwCMGVAsKlUCEQUY8lmAkBiICYKDgo5m0eAwMSCf6HGbNZXIrSx9rzTQwLMCBVewNGaXr/zudexWzz5hzHL/x1+Ou//r0bRZzoeNQuLD3dltS2bOvHhGAu56nt/////zbqYYaqdpznop51v+tFSVPcwqUKHjYwVHE2OA+4dR+v1rbi9SUza3cEEIW9JArATS//OExM4waz5oAObOvADW1V2bkOyVMEGC3WaoDIkDEOBpMltTbwwLk8QFBSCz1hB0AzEiBgsK4ytptUxcZu4ABnTLFtWkGfA6X0XAWtJ1MzIgay8xwol9FopAE9dpItf7O47739b5vXf13/1///b06sZBxQpLt21amXig6lZp/////98rJNr0pR2n30fU6oWhBSoooGY50K5zDBpSI4YkyR//O2LrUNMLCEUgDliQQhPMB/wMGoNCg8b+ls4GQMQf//OExL8pA15oAOaEvCZUCo6A0ZMqHCzrZjB4RJyk4YEWULE+GtAUiX5cQuuGHVPI9GADAYciKFyKfSCqG4GFNKW6jsydOt00CbNn7pZRhX/XcNd5z/5/8////////2//dlMRCXX6aNoyhBDt//////+v//rqjnYcqHAQRXWrClIDAYLFg7kX///uomEpAlQzOhykF2VmpcJnoKEBBhY6fmQF8QAGG0Xgkdt6I0EebXZIDxFoxNQUyKQg1tHQTyW6//OExM4mM0JsAN6EvP6XVd2aYOzJt0qwSMSDkQJmBJDvGzMMFcmGBgZQpnj+SKpei2tVd/vWH///3X///3v//v9f6GfQRQVHqRCnZ2o9VFxgCgiIrf+n9v/////62occJCImzsrbDzqwgiB8XFRIOFuWLuKiDOIiiDQMn3f+jS0ZRVBQDFggeIJYsXAgNGIpIJHowsGTCxjMsgALBcwUgjFxxEgAl2YcEYYGURRnM9DwSU08oiTjhyMPuuemYJCp//OExOgqO4pkAN5KvFqUt0bqYTJlKoTQCYAuoSx8wgQ6iEDiYZLLnQea9y12rnhh/8x/+9/eP8/9////+zfej1aUPFJWfQ4w56FS5iMwLReQVH2//7f/////3aap00eYo6lRy5BTFMUweQXDgrB6KxIEY8fOMUmNwlLlyJ6VLkXgsyK/lCTEH2AsGQwQAoVuju30BMRf41+xMGAViGFFAKbkbQLPHMiwkLN0QlLxCwzQkWCIxwlZchSZSbW2iu5D//OExPIu+55cAOZOvEuNJouwyZWRD0BSMD3FJ9GWK01drSZCWvAxkAANCtaQvrKsp/teau/rmt6y7ruPf/v81/+3azf51B52CEsJlc9yKMaiIzjwLQWiWYcTPO/6t85v//////6TpQ+zGmu2/NHjmHzTKjpqHDs4cLDUWtzLVS9FDt1qamEGBAAYfKh3mGGMQsooZqDYYDwuFjAYAIhsX9MIjgxmGFalOofWin7KVcBUMtJS8tC8jzKrushxsrNT//OExOkrU45UAN4OvA2HS8EBEQTCgjBgDAJzQLgicc8gaaeaUwYlYNSlll4QSYedBC+tPnDcPAWJpWBlO3G18/PDNa/8cRHPHfTWUcGQ8GwbeHztfUDRcBoeh8eGgbA+CvvRjTtVF/9nSY4GEQUexLMOm2LCQVMgIRBrsEi2oMJCrkFh0UogAF8R02dhWsUx7wJBIZQchcpiLFTFHTCAXHep2YUzVead7MHUcZm6/ZYy6F07aqYL/a4wVfiAYYCD//OExO4s4fZEAOaQmCPL8igIyb42q4e/luRoeVTgFQAdkY1Emal8mZQxKG68NT27mXcNZc1+u9/WXOa7/2Z81B9DLOcYqlTR44uQUiWYqxzHzRUBUGkgKHCQqlzWT9/TTWx1DFTTdf9raf1/zjlrd12f/6o6Z6x2XOqW6KkVC6bqqhAQHZBJDPouWjINWWAaWuUuSQKL6Fom5OglsjEXORiTWSTg6HFU2AqEPzACtzvuS7iz3NXYnU8cRLQkwIGG//OExO0rk448ANaOvEnzMhOy1QYkNYgCCzMLBAKrHamnyYnETIxYKzi52m45r32+tppekx7vTZqbVatWbmSa0D8Kc1tf7fmxdNXXIxKZGLtjFpOPKrC4Sq3MII6lJ6sEb4MXNXfg/zFZOa2lMAYoDxS3ZZyu2YXfSlWK+rdEfa+58c+JPZO/9xwTIoKYU8LWZRKmysCV1G30ksZhpTaBWWy1LaflWV3HQSE3UMRWvUoijSMs1DzN1SKQWIwwMory//OExPEuOfIsAMZYmYwE0IyXwTKOjk8nbzRo2a4z92xnOcX/+7b184xv7xv4xPb1w+NNfKW2NCetrKo5nQNhqHRVOzv7zvjf371uzYU2VnXboOmV7hq1qeMLfs+XD5P1+7NW1WIz9P3WvvPhtxoPp4mNx57ZPtr+Kmwk6J9XaZUFRhrLPwUWK2J8r7pReZqXaZ+4jKI7MPxTw7GLa0WjLy0rwuHTfHyuGq9u4wOiovXNor34oC0QUrkOpyatS8z1//OExOssM7IkAMPM3eYp333F3ehVcfY2L4tYihr+Mz8f/OynUMlobJFfqbTnqKt9jlllFGjDiG3jUxT2lGXR4w1JyG1ZzfSI4iSVSGnWVXSnD5N2nO1VGTXpHa2/dM9LQLdGWaSkL1xUprzTyy2KLaNuk8cyEpOr96pmjNKpkW9/ULXZzAoWiBoy6XRivZp7li3dll2M23hdylgByn7EpwjodggTSWVFZ09N0ThVFMdU0lZ1tRFpZFSFEnalqqRk//OExO0sxBoUAMMM3dJQTQ7OS9JrYsmyhrGlFLXucnbKJxGKgLZS6RrDTa9JG5+gH6emEDTWXheKVvfxdkkL6JErJRNQPFdHXdKmtvZIww6zLl1V5g1VPTlybCVemuKZn9GVLQ1z99TltfORnUJ1JArKY+FZqJEotfinyb+HF/HRKxhsyidRgTToNna2W7dveMelU9apZ+LSiLRE9pUlFU4E0yW0tMPMDQ+iFJdoVSlRjFaUQMrrKoUMFqIkUSWk//OExO0sXBoQAMJM3ZulmY33eMUmECh9dD6i2yq/xkeBTBVS3ad0lhlFTVVJmVBp5AdhFtLjEJaS04sKI7Dkcw58miJBJYkos6GfF+FMmRmY02kGYgU0RVSBNNd8dnI7LN7sjj4bD/zmUU1WbiTuWbcpSjQoxutMpfOQNiZ0dZ6Zx7k8SYgpETv3sc8cK8rsTVBNZ0r0Sm9nbnzTLBESEjZRERFpnWe4oGAkukPSeSoBxEr0LLcSuthCZq9LEjel//OExO4tBCH8AMpM3fSR5OrHDLaDJNwfCZcqhjCT7Q6pSpOUHkoxSVqyR9R8avArLjNgoyiLGCAglDeFpTiALGrojJtebJGkV2dck4pK9fV9Odrmn221CingrIpCnpz8q5SKTiuYjZR6dHDqfd/1I4kdBslsnroWby1moeCWFVhaDRSiFesPmSopxrVaLLt3Ktne1R/P1abDLO3TxSZgxuh8nlZek1ydW0oNtyRESBW1Y9h6yiTBGuwlRttdqSJh//OExO0tdDn0ANJM3AtQyCyjKFhO2V0ZaTJ4uiptdhVwLWnS5diitPeJ5GEvifprGIQRPfH+LvY2NlsOXV5bowb62Xgohse3mEuS56JbsuGUXEyiWWrxNmJHP0oqDZmIS64xcKeCuuOqWp0r3GTzUXhG/J1FThDWYtz6yyKPjnXZFSuplWo6W9bs2e67crWJZGbnK83MEyZ4tGCtkRH0AIsKg2pQJQR4LCdpy8TokGhnUaxGkVVME0Fmg8xNsQBc//OExOoq7Cn4AMpM3WII0RQ4gXX3lTTfJ0CiFCYWbRWA1GPbnvJJIYUhEWaKURqj1I3yGPaA+dyq0gKclfTWxLmnSlj0WQM2hEmIkuUeVEQem8lCOVh8JRbE6KMygTCR5WG7VZEoNNnIVPXCj7lFAdDlQX49Guptjly4UKQYXiSEPh3KNMUhUZMKwGUpEbs35ZhOyu7dv4X8LOdeT0VWZtKCnBz+MK+WxUGi88VICpHeE7xll8+yKGzDJzRk4qhF//OExPEvJDHwANJM3aw4WI2WyoTk7aAVGYcjmfraYLSx8uOnyh1fPk9RZJSoIFxNrdPS6TIm5mwbEwydbdJl1c5ZprtBZvVNVSDTJOs8Hu2JOdMm1LmOvKIGUeyybJZDnUXVaTKRLkMlCDuQVTHcudUm5X+L73Zv+InMUXhWty2ajYw+MPkwPOvLJ0CKK41+znnrGtXocrcrlVbt2WfYs2z6HIpkTRvfMnUISzUYk82CN4kpCSIhHaGRAwworE63//OExOcs3C30ANMM3SPyJlmCdWkeelcyCklSWBA3ZkpepzRpeCC9KLInoI5SvihLcSVu0Ix6xim1RO/6o5CDpnUyi5PjlKhPFy0pUCsMX4o1DCGsjp2FzolM4XSRTYtFVJmxacKZ6x0cm3aDMOxi0NNvNw2jEItpKI1LzkRsp2tGorIqLre1+nRIrycTS5as3srOdS/LK3ynuF2lpe2YroFtUapHJtITICJOYnkfXD2vXcWSxhVtEkwZbVg2ubxB//OExOYsPCn0ANJM3TUaeUB9yUVCBBrTZIjRcUMjporXtJjE12xFX8EijE+pDUKo/Doz1+as1MsdR7Auqb0Y2rMS/cWYVEIXKTLP6FsLKJGI1iJNia82DEwOzdxNPwiXuSiW0l/tLtnXh94IuW5ZDNNIc7GpfjCk3YynlMqMo8pBl3mFkEkkKvUsdDsQKRGrWVzLuNmpjcq0MavVabK79ZllH2lhVqs4SOETbAKubRFrMoWIVGSNQuyinSxldWyY//OExOgtRDn0ANJM3MNRPJo2hGiQEL0KwoRlP44SxR25pE3NtbNMPBhZ1qtdESYTg9SpQpr8c7/A4yKyp9nNKNPZplS6J7PUGGY+0cpeTOOE20mYe0HYa7tdR1gdTNHNGHEiJEHgl/JEx5uD+km4RjXZsJ3MkHRJXmH7z/ajQSbLMqNPz3hZlf5FQuiCCinC/zdresrWOVymqxXVBavR34jAqcRqahZEWkM86K8PttIjUcmr0KSJpa2VmCRHbCcC//OExOYs3Dn0ANJM3IjR1FiC4dNrVMk5cohtSLDCy8jTaqNuaiIVDFEqLP1CNknklbklzuyY0tEwdt34g3UbhO2bXnIQ1GZuH8W9YlrITeHNiUP4cox/GRJ3NQjnbjq6odofWa67Ln/n1UofXZ1cxU95+MYVM55Zi8flFTlWtqXn/5cn07sS/GMV5PuUyqkh+VQq0+FJjdsvwqXpyQXW9MXiwcVNjoqYHRXh0SQbMCMWS2mxGP6dfqgvlUkCPaxd//OExOUp/DX4AMpM3FkZDhlOJaMQS0eGTxB1BNoYVIGCqdLhQdI4EqlKl5IZHuOKzRxIPCx9gktsPElL12WIqa4kgq/Ezh6pMXLJHmWXFTTRVSWl1yF95lG4tq6sK0FFDMrKWpsOL+unahYbQ32n1vOmetnKEu3rehXivXtOFsul/V9odPmxnry6jlENDyfW5Z9Z91Khjqt9VIZsnFdWLG5OFTLzx6ba6ssr1crx5ScTRXp7PlmzvHKpaxmPlcYt//OExPA3pDnoANMY3Mow9ZUmSYEjKDlZI6ioSK2MnydYmmwpJEWZrXGolUcFiUgbEg8So0Dm4+uWe0IXqrI2nwdGTSF8NivEAE3TIpveiikmK7zLli7IB1scvXWOZLbwVJ2XdXrse20yOMb8uXjcJuWtFNA9A2U3lNlL3FDSGlkCtPgzdS6cOhOQgW5haVdJCi16RT3NvJtstkH9pzJN9R04DOidqbzjxPf10/DNZ9orjWavb2srtvly5n2zjM2s//OExMQs7DX0ANJM3G1Q4FydpoUtnpRxweqydEjUFBLYYXLW3pcgFdtOXOztmiUojhiEhkRETLSRllQnumhXJcgR5kJykmhtHc4FMehh+Ybkmiriisx5bMKM27Ne2fzfX01lc/htLPdzGyiqUPbqYx9M0wrOUPi9LNMOV2JFGoJcnczA1P7RlzGSc1791GjFm0QLybW0Fnmx0fphumGwxkKd5wo1ko9QU6mSa066UGoGOmRRWfcxW1zdaSBmV1LY//OExMMslDH0ANJM3by+XUXW2iMJJoHplEhQ5GLnhLFuLSSaeLTbXaJLLTgrsEPSiRrspZJFN0Wa6SGuyzJHUpQV0RgsjkWMUeljyxpCTTLYmVx1VuVclKjmuWWO2Y6p0H2ryNZbJHuTbstow6q9xg83xqN6tflwOEdS7Nc26yEa6pU3aTakkfdQbQyyLbauOeT6VN6Qo+Ygbx8VZcDTWPCBFThdQJjjT3YlcugWXyuUVbVFR3O2rWFDMUFGDkAC//OExMMpXBH4AKJQ3TBzMkE2YQFqNnAUCsuaNIoJYXVyRFFyYDh4p40q0TYNbpYaVGUUfBw4aR2W7OqscOx0HNVmsUEgsTUMM/hhZM6ONDlQCFBSlgFcLAIqTA0HVQwdKymQqxaVDPsQMT+voaPNtcmCkFVfXpUgoYUTo8s7xmbZmOGR4ZZYZGs4DQKCxRLqtXYKM9hVoOAPDSZUuWKwJnrd3UdZ/ZJG5NDz6wTGJHOkxCcXYyScHoURCNFCcyiV//OExNAovDn0AMGG3FHsrJTyUZXmyuE2WlVjq7DUanjSqcHsrJw3IyTUbciISxxG5qNarRyNWstI1/+WVDJgtQyNWsjkf//5kayGUsNWjoYlgoIG5MscYCCg0cjuyrKjkyyGX5rI5HZk0sqGrWkf/2//y3P8mWx0NWCggTk1hqGqTEFNRTMuMTAwqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq//OExOAoFDm0AMJG3Kqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq\" type=\"audio/mpeg\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}