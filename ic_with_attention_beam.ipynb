{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sqxGzM4rSnCj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "5c3c0fee-2263-467c-a662-a86b80a91216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2.0.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.1.1 sacrebleu-2.5.1\n",
            "Collecting gTTS\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from gTTS) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.11/dist-packages (from gTTS) (8.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (2025.1.31)\n",
            "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: gTTS\n",
            "Successfully installed gTTS-2.5.4\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.21.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.2 (from gradio)\n",
            "  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.21.0-py3-none-any.whl (46.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.11 ffmpy-0.5.0 gradio-5.21.0 gradio-client-1.7.2 groovy-0.1.2 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.0 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sacrebleu\n",
        "!pip install gTTS\n",
        "!pip install nltk\n",
        "!pip install tqdm\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwvmPmzpaACW",
        "outputId": "a37a4343-5bad-48fc-871a-18e9cb56256a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dlJqXmpNTEKM",
        "outputId": "32331be2-56d8-4a17-fbd7-46ff9020f311"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import nltk\n",
        "from PIL import Image\n",
        "import sacrebleu\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download NLTK tokenizer data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Set device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTa-t9UhUbc1",
        "outputId": "640506e4-f8aa-4dc2-f0c6-b7cfd8e6dd9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 3006\n",
            "Total images found: 8091\n",
            "Total images found: 8091\n",
            "Total images found: 8091\n",
            "Train, val, test sizes: 6472 647 972\n"
          ]
        }
      ],
      "source": [
        "# Vocabulary class (as in the repository)\n",
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.freq_threshold = freq_threshold\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {v: k for k, v in self.itos.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer_eng(text):\n",
        "        return [tok.lower() for tok in word_tokenize(text)]\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = {}\n",
        "        idx = 4\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer_eng(sentence):\n",
        "                frequencies[word] = frequencies.get(word, 0) + 1\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokenized_text = self.tokenizer_eng(text)\n",
        "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in tokenized_text]\n",
        "\n",
        "\n",
        "# Custom Dataset with train/val/test splitting (for .txt file with CSV format)\n",
        "class FlickrDataset(Dataset):\n",
        "    def __init__(self, root_dir, captions_file, vocabulary, transform=None, split=\"train\", split_ratio=(0.80, 0.08, 0.12)):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir: Directory with images.\n",
        "            captions_file: Path to the .txt file with captions.\n",
        "                           Expected format:\n",
        "                           image,caption\n",
        "                           1000268201_693b08cb0e.jpg,A child in a pink dress is climbing up a set of stairs in an entry way .\n",
        "                           ...\n",
        "            vocabulary: A Vocabulary object.\n",
        "            transform: Image transformations.\n",
        "            split: One of 'train', 'val', or 'test'.\n",
        "            split_ratio: Tuple for train/val/test splits.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.vocabulary = vocabulary\n",
        "\n",
        "        # Read the file and split lines\n",
        "        with open(captions_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        imgs = []\n",
        "        caps = []\n",
        "        # Skip header if present (assuming first line starts with \"image\")\n",
        "        if lines[0].strip().lower().startswith(\"image\"):\n",
        "            lines = lines[1:]\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            parts = line.split(',', 1)  # split into two parts at the first comma\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            img_name = parts[0].strip()\n",
        "            caption = parts[1].strip()\n",
        "            imgs.append(img_name)\n",
        "            caps.append(caption)\n",
        "\n",
        "        # Map images to their captions\n",
        "        self.img2caps = {}\n",
        "        for img, cap in zip(imgs, caps):\n",
        "            if img not in self.img2caps:\n",
        "                self.img2caps[img] = []\n",
        "            self.img2caps[img].append(cap)\n",
        "        self.imgs = list(self.img2caps.keys())\n",
        "        print(\"Total images found:\", len(self.imgs))\n",
        "\n",
        "        # Split dataset into train/val/test\n",
        "        random.seed(42)\n",
        "        random.shuffle(self.imgs)\n",
        "        total = len(self.imgs)\n",
        "        train_end = int(split_ratio[0] * total)\n",
        "        val_end = train_end + int(split_ratio[1] * total)\n",
        "\n",
        "        if split == \"train\":\n",
        "            self.imgs = self.imgs[:train_end]\n",
        "        elif split == \"val\":\n",
        "            self.imgs = self.imgs[train_end:val_end]\n",
        "        elif split == \"test\":\n",
        "            self.imgs = self.imgs[val_end:]\n",
        "        else:\n",
        "            raise Exception(\"split must be one of 'train', 'val', or 'test'\")\n",
        "\n",
        "        self.split = split\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_id = self.imgs[index]\n",
        "        caps = self.img2caps[img_id]\n",
        "        # For training, pick a random caption; for validation/testing, use the first caption\n",
        "        caption = random.choice(caps) if self.split == \"train\" else caps[0]\n",
        "        img_path = os.path.join(self.root_dir, img_id)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        # Add start and end tokens\n",
        "        numericalized_caption = [self.vocabulary.stoi[\"<SOS>\"]]\n",
        "        numericalized_caption += self.vocabulary.numericalize(caption)\n",
        "        numericalized_caption.append(self.vocabulary.stoi[\"<EOS>\"])\n",
        "        return image, torch.tensor(numericalized_caption)\n",
        "\n",
        "# Collate function to pad sequences\n",
        "def collate_fn(batch):\n",
        "    images, captions = zip(*batch)\n",
        "    images = torch.stack(images, 0)\n",
        "    captions = pad_sequence(captions, batch_first=True, padding_value=0)\n",
        "    return images, captions\n",
        "\n",
        "# InceptionV3 expects 299x299 images.\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),           # New augmentation\n",
        "    transforms.RandomRotation(10),                    # New augmentation\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # New\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Keep original transform for validation/test\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "# Build vocabulary from the captions file (skip header if present)\n",
        "def build_vocab(captions_file, freq_threshold):\n",
        "    with open(captions_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    # Skip header line if it starts with \"image\"\n",
        "    if lines[0].strip().lower().startswith(\"image\"):\n",
        "        lines = lines[1:]\n",
        "    captions = []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        parts = line.split(',', 1)\n",
        "        if len(parts) < 2:\n",
        "            continue\n",
        "        captions.append(parts[1].strip())\n",
        "    vocab = Vocabulary(freq_threshold)\n",
        "    vocab.build_vocabulary(captions)\n",
        "    return vocab\n",
        "\n",
        "# Update these paths to your dataset locations\n",
        "captions_path = \"/content/drive/MyDrive/Flickr/captions.txt\"\n",
        "images_root = \"/content/drive/MyDrive/Flickr/Images\"\n",
        "\n",
        "# Build vocabulary (adjust frequency threshold as needed)\n",
        "vocab = build_vocab(captions_path, freq_threshold=5)\n",
        "print(\"Vocabulary size:\", len(vocab))\n",
        "num_workers = 0\n",
        "\n",
        "# Create dataset objects for train, validation, and test splits\n",
        "train_dataset = FlickrDataset(root_dir=images_root, captions_file=captions_path, vocabulary=vocab, transform=train_transform, split=\"train\")\n",
        "val_dataset   = FlickrDataset(root_dir=images_root, captions_file=captions_path, vocabulary=vocab, transform=val_test_transform, split=\"val\")\n",
        "test_dataset  = FlickrDataset(root_dir=images_root, captions_file=captions_path, vocabulary=vocab, transform=val_test_transform, split=\"test\")\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn, num_workers=num_workers)\n",
        "val_loader   = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)\n",
        "test_loader  = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)\n",
        "\n",
        "print(\"Train, val, test sizes:\", len(train_dataset), len(val_dataset), len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m0Qgktmag0nF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import inception_v3, Inception_V3_Weights\n",
        "\n",
        "from torchvision.models import inception_v3, Inception_V3_Weights\n",
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        # Instantiate InceptionV3 with default weights and aux_logits=True (as required by the weights API)\n",
        "        inception = inception_v3(weights=Inception_V3_Weights.DEFAULT, aux_logits=True)\n",
        "        # Override the avgpool and fc layers so they don't alter the feature map.\n",
        "        inception.avgpool = nn.Identity()\n",
        "        inception.fc = nn.Identity()\n",
        "        # Manually assemble the layers up to Mixed_7c using the correct attribute names.\n",
        "        self.features = nn.Sequential(\n",
        "            inception.Conv2d_1a_3x3,\n",
        "            inception.Conv2d_2a_3x3,\n",
        "            inception.Conv2d_2b_3x3,\n",
        "            inception.maxpool1,             # previously MaxPool_3a_3x3\n",
        "            inception.Conv2d_3b_1x1,\n",
        "            inception.Conv2d_4a_3x3,\n",
        "            inception.maxpool2,             # previously MaxPool_4a_3x3\n",
        "            inception.Mixed_5b,\n",
        "            inception.Mixed_5c,\n",
        "            inception.Mixed_5d,\n",
        "            inception.Mixed_6a,\n",
        "            inception.Mixed_6b,\n",
        "            inception.Mixed_6c,\n",
        "            inception.Mixed_6d,\n",
        "            inception.Mixed_6e,\n",
        "            inception.Mixed_7a,\n",
        "            inception.Mixed_7b,\n",
        "            inception.Mixed_7c\n",
        "        )\n",
        "        # Linear projection: project 2048-dim features to embed_size for each spatial location.\n",
        "        self.linear = nn.Linear(2048, embed_size)\n",
        "\n",
        "    def forward(self, images):\n",
        "        # images: (batch, 3, 299, 299)\n",
        "        x = self.features(images)  # Expected shape: (batch, 2048, H, W), e.g., (batch, 2048, 8, 8)\n",
        "        batch_size, C, H, W = x.size()\n",
        "        # Reshape to (batch, num_pixels, 2048) where num_pixels = H * W.\n",
        "        x = x.view(batch_size, C, -1).permute(0, 2, 1)\n",
        "        # Project each spatial feature into the embedding space.\n",
        "        x = self.linear(x)  # (batch, num_pixels, embed_size)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ----- Attention Module -----\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # transforms encoder features\n",
        "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # transforms decoder hidden state\n",
        "        self.full_att = nn.Linear(attention_dim, 1)               # computes scalar attention energy\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, encoder_out, decoder_hidden):\n",
        "        # encoder_out: (batch, num_pixels, encoder_dim)\n",
        "        # decoder_hidden: (batch, decoder_dim)\n",
        "        att1 = self.encoder_att(encoder_out)               # (batch, num_pixels, attention_dim)\n",
        "        att2 = self.decoder_att(decoder_hidden).unsqueeze(1) # (batch, 1, attention_dim)\n",
        "        att = self.full_att(self.relu(att1 + att2)).squeeze(2) # (batch, num_pixels)\n",
        "        alpha = self.softmax(att)                          # (batch, num_pixels)\n",
        "        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch, encoder_dim)\n",
        "        return context, alpha\n",
        "\n",
        "# ----- Decoder with Attention and Beam Search -----\n",
        "class DecoderRNNWithAttention(nn.Module):\n",
        "    def __init__(self, embed_size, decoder_dim, vocab_size, num_layers, attention_dim):\n",
        "        \"\"\"\n",
        "        embed_size: dimension of word embeddings and encoder output projection.\n",
        "        decoder_dim: LSTM hidden size.\n",
        "        vocab_size: number of tokens.\n",
        "        num_layers: number of LSTM layers.\n",
        "        attention_dim: dimension used in attention module.\n",
        "        \"\"\"\n",
        "        super(DecoderRNNWithAttention, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.attention = Attention(encoder_dim=embed_size, decoder_dim=decoder_dim, attention_dim=attention_dim)\n",
        "        # Input dimension to LSTM: word embedding (embed_size) + context vector (embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size + embed_size, decoder_dim, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(decoder_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.decoder_dim = decoder_dim\n",
        "\n",
        "    def forward(self, encoder_out, captions):\n",
        "        \"\"\"\n",
        "        Teacher forcing: captions is ground-truth. We do not incorporate attention in a vectorized\n",
        "        way for all time steps in this simple implementation.\n",
        "        \"\"\"\n",
        "        embeddings = self.embed(captions[:, :-1])  # (batch, seq_len, embed_size)\n",
        "        batch_size = encoder_out.size(0)\n",
        "        seq_len = embeddings.size(1)\n",
        "        outputs = []\n",
        "        hidden, cell = None, None\n",
        "        for t in range(seq_len):\n",
        "            # For the first time step, or if hidden is not defined, use zeros.\n",
        "            if hidden is None:\n",
        "                hidden_state = torch.zeros(batch_size, self.decoder_dim).to(encoder_out.device)\n",
        "            else:\n",
        "                hidden_state = hidden[-1]  # (batch, decoder_dim)\n",
        "            # Compute attention context vector\n",
        "            context, alpha = self.attention(encoder_out, hidden_state)\n",
        "            # Get embedding for current word\n",
        "            word_emb = embeddings[:, t, :]  # (batch, embed_size)\n",
        "            lstm_input = torch.cat([word_emb, context], dim=1).unsqueeze(1)  # (batch, 1, 2*embed_size)\n",
        "            output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell)) if hidden is not None else self.lstm(lstm_input)\n",
        "            output = self.linear(output.squeeze(1))\n",
        "            outputs.append(output)\n",
        "        outputs = torch.stack(outputs, dim=1)\n",
        "        return outputs\n",
        "\n",
        "    def sample(self, encoder_out, max_len=20):\n",
        "        \"\"\"Greedy decoding with attention (for batch size = 1).\"\"\"\n",
        "        batch_size = encoder_out.size(0)\n",
        "        assert batch_size == 1, \"sample() supports only batch_size = 1\"\n",
        "        sampled_ids = []\n",
        "        hidden, cell = None, None\n",
        "        # Start with <SOS> token.\n",
        "        inputs = self.embed(torch.tensor([vocab.stoi[\"<SOS>\"]], device=encoder_out.device)).unsqueeze(1)\n",
        "        for t in range(max_len):\n",
        "            if hidden is None:\n",
        "                hidden_state = torch.zeros(1, self.decoder_dim).to(encoder_out.device)\n",
        "            else:\n",
        "                hidden_state = hidden[-1]\n",
        "            context, alpha = self.attention(encoder_out, hidden_state)\n",
        "            lstm_input = torch.cat([inputs.squeeze(1), context], dim=1).unsqueeze(1)\n",
        "            output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell)) if hidden is not None else self.lstm(lstm_input)\n",
        "            output = self.linear(output.squeeze(1))\n",
        "            predicted = output.argmax(1)\n",
        "            sampled_ids.append(predicted.item())\n",
        "            if predicted.item() == vocab.stoi[\"<EOS>\"]:\n",
        "                break\n",
        "            inputs = self.embed(predicted).unsqueeze(1)\n",
        "        return sampled_ids\n",
        "\n",
        "    def sample_beam(self, encoder_out, beam_size=3, max_len=20):\n",
        "        \"\"\"\n",
        "        Beam search decoding with attention for batch_size = 1.\n",
        "        Returns the best sequence as a list of token ids.\n",
        "        \"\"\"\n",
        "        k = beam_size\n",
        "        vocab_size = self.linear.out_features\n",
        "        # Start with <SOS> token.\n",
        "        start_token = torch.tensor([vocab.stoi[\"<SOS>\"]], device=encoder_out.device)\n",
        "        inputs = self.embed(start_token).unsqueeze(1)  # (1, 1, embed_size)\n",
        "        # Initialize hidden state with zeros.\n",
        "        hidden, cell = None, None\n",
        "        # For initial step, define a zero hidden state for attention.\n",
        "        hidden_state = torch.zeros(1, self.decoder_dim).to(encoder_out.device)\n",
        "        context, alpha = self.attention(encoder_out, hidden_state)\n",
        "        lstm_input = torch.cat([inputs.squeeze(1), context], dim=1).unsqueeze(1)\n",
        "        output, (hidden, cell) = self.lstm(lstm_input, None)\n",
        "        output = self.linear(output.squeeze(1))\n",
        "        log_probs = torch.log_softmax(output, dim=1)\n",
        "        topk_log_probs, topk_indices = log_probs.topk(k)\n",
        "        beams = []\n",
        "        for i in range(k):\n",
        "            token = topk_indices[0, i].unsqueeze(0)\n",
        "            score = topk_log_probs[0, i].item()\n",
        "            beams.append((token, score, hidden, cell))\n",
        "        for t in range(max_len - 1):\n",
        "            candidates = []\n",
        "            for seq, score, hidden, cell in beams:\n",
        "                if seq[-1].item() == vocab.stoi[\"<EOS>\"]:\n",
        "                    candidates.append((seq, score, hidden, cell))\n",
        "                    continue\n",
        "                last_token = seq[-1].unsqueeze(0)\n",
        "                emb = self.embed(last_token).unsqueeze(1)\n",
        "                hidden_state = hidden[-1]\n",
        "                context, alpha = self.attention(encoder_out, hidden_state)\n",
        "                lstm_input = torch.cat([emb.squeeze(1), context], dim=1).unsqueeze(1)\n",
        "                output, (new_hidden, new_cell) = self.lstm(lstm_input, (hidden, cell))\n",
        "                output = self.linear(output.squeeze(1))\n",
        "                log_probs = torch.log_softmax(output, dim=1)\n",
        "                topk_log_probs, topk_indices = log_probs.topk(k)\n",
        "                for i in range(k):\n",
        "                    new_token = topk_indices[0, i].unsqueeze(0)\n",
        "                    new_seq = torch.cat([seq, new_token])\n",
        "                    new_score = score + topk_log_probs[0, i].item()\n",
        "                    candidates.append((new_seq, new_score, new_hidden, new_cell))\n",
        "            beams = sorted(candidates, key=lambda tup: tup[1], reverse=True)[:k]\n",
        "            if all(seq[-1].item() == vocab.stoi[\"<EOS>\"] for seq, _, _, _ in beams):\n",
        "                break\n",
        "        best_seq = beams[0][0]\n",
        "        return best_seq.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "8qGXzHing2-Q"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "def evaluate(encoder, decoder, criterion, data_loader, device):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(data_loader, desc=\"Evaluating\", leave=False)\n",
        "        for images, captions in pbar:\n",
        "            images, captions = images.to(device), captions.to(device)\n",
        "            features = encoder(images)\n",
        "            outputs = decoder(features, captions)\n",
        "            # Fixed version - use shifted captions for targets\n",
        "            loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions[:, 1:].reshape(-1))\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix({\"val_loss\": loss.item()})\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    return avg_loss\n",
        "\n",
        "def train(encoder, decoder, criterion, optimizer, train_loader, val_loader, num_epochs, device, save_path=\"Downloads/Flickr/best_model.pth\"):\n",
        "    encoder.to(device)\n",
        "    decoder.to(device)\n",
        "    best_val_loss = float('inf')\n",
        "    # Add LR scheduler\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        encoder.train()\n",
        "        decoder.train()\n",
        "        running_loss = 0.0\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
        "        for images, captions in pbar:\n",
        "            images, captions = images.to(device), captions.to(device)\n",
        "            features = encoder(images)\n",
        "            outputs = decoder(features, captions)\n",
        "            loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions[:, 1:].reshape(-1))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            pbar.set_postfix({\"Loss\": loss.item()})\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        avg_val_loss = evaluate(encoder, decoder, criterion, val_loader, device)\n",
        "\n",
        "        # ====== ADD SCHEDULER STEP HERE ======\n",
        "        scheduler.step(avg_val_loss)  # Update learning rate based on validation loss\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Save checkpoint if validation loss decreases\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save({\n",
        "                'encoder_state_dict': encoder.state_dict(),\n",
        "                'decoder_state_dict': decoder.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'epoch': epoch,\n",
        "                'val_loss': avg_val_loss,\n",
        "            }, save_path)\n",
        "            print(f\"Checkpoint saved at epoch {epoch+1} with val loss {avg_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3572d7f5-0314-43d7-bf43-09d5965f095b",
        "collapsed": true,
        "id": "5cTdgZQOqgbR"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Loss: 5.2917, Val Loss: 4.7495\n",
            "Checkpoint saved at epoch 1 with val loss 4.7495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/100], Train Loss: 4.5492, Val Loss: 4.3383\n",
            "Checkpoint saved at epoch 2 with val loss 4.3383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/100], Train Loss: 4.2770, Val Loss: 4.1528\n",
            "Checkpoint saved at epoch 3 with val loss 4.1528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/100], Train Loss: 4.1208, Val Loss: 4.0250\n",
            "Checkpoint saved at epoch 4 with val loss 4.0250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/100], Train Loss: 3.9827, Val Loss: 3.9135\n",
            "Checkpoint saved at epoch 5 with val loss 3.9135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/100], Train Loss: 3.8923, Val Loss: 3.8531\n",
            "Checkpoint saved at epoch 6 with val loss 3.8531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/100], Train Loss: 3.8073, Val Loss: 3.7827\n",
            "Checkpoint saved at epoch 7 with val loss 3.7827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/100], Train Loss: 3.7465, Val Loss: 3.7374\n",
            "Checkpoint saved at epoch 8 with val loss 3.7374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/100], Train Loss: 3.6902, Val Loss: 3.6853\n",
            "Checkpoint saved at epoch 9 with val loss 3.6853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Train Loss: 3.6528, Val Loss: 3.6621\n",
            "Checkpoint saved at epoch 10 with val loss 3.6621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/100], Train Loss: 3.5933, Val Loss: 3.6267\n",
            "Checkpoint saved at epoch 11 with val loss 3.6267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/100], Train Loss: 3.5561, Val Loss: 3.6131\n",
            "Checkpoint saved at epoch 12 with val loss 3.6131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/100], Train Loss: 3.5280, Val Loss: 3.5945\n",
            "Checkpoint saved at epoch 13 with val loss 3.5945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/100], Train Loss: 3.5053, Val Loss: 3.5659\n",
            "Checkpoint saved at epoch 14 with val loss 3.5659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/100], Train Loss: 3.4696, Val Loss: 3.5522\n",
            "Checkpoint saved at epoch 15 with val loss 3.5522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/100], Train Loss: 3.4579, Val Loss: 3.5346\n",
            "Checkpoint saved at epoch 16 with val loss 3.5346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/100], Train Loss: 3.4151, Val Loss: 3.5156\n",
            "Checkpoint saved at epoch 17 with val loss 3.5156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/100], Train Loss: 3.3961, Val Loss: 3.5072\n",
            "Checkpoint saved at epoch 18 with val loss 3.5072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/100], Train Loss: 3.3688, Val Loss: 3.4929\n",
            "Checkpoint saved at epoch 19 with val loss 3.4929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/100], Train Loss: 3.3411, Val Loss: 3.4915\n",
            "Checkpoint saved at epoch 20 with val loss 3.4915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [21/100], Train Loss: 3.3330, Val Loss: 3.4800\n",
            "Checkpoint saved at epoch 21 with val loss 3.4800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [22/100], Train Loss: 3.3051, Val Loss: 3.4735\n",
            "Checkpoint saved at epoch 22 with val loss 3.4735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [23/100], Train Loss: 3.2913, Val Loss: 3.4621\n",
            "Checkpoint saved at epoch 23 with val loss 3.4621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [24/100], Train Loss: 3.2755, Val Loss: 3.4658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [25/100], Train Loss: 3.2667, Val Loss: 3.4588\n",
            "Checkpoint saved at epoch 25 with val loss 3.4588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [26/100], Train Loss: 3.2549, Val Loss: 3.4432\n",
            "Checkpoint saved at epoch 26 with val loss 3.4432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [27/100], Train Loss: 3.2399, Val Loss: 3.4437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [28/100], Train Loss: 3.2174, Val Loss: 3.4492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [29/100], Train Loss: 3.1949, Val Loss: 3.4334\n",
            "Checkpoint saved at epoch 29 with val loss 3.4334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [30/100], Train Loss: 3.1801, Val Loss: 3.4308\n",
            "Checkpoint saved at epoch 30 with val loss 3.4308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [31/100], Train Loss: 3.1828, Val Loss: 3.4348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [32/100], Train Loss: 3.1512, Val Loss: 3.4310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [33/100], Train Loss: 3.1516, Val Loss: 3.4113\n",
            "Checkpoint saved at epoch 33 with val loss 3.4113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [34/100], Train Loss: 3.1268, Val Loss: 3.4127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [35/100], Train Loss: 3.1271, Val Loss: 3.4123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [36/100], Train Loss: 3.1043, Val Loss: 3.4146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [37/100], Train Loss: 3.0660, Val Loss: 3.4064\n",
            "Checkpoint saved at epoch 37 with val loss 3.4064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [38/100], Train Loss: 3.0536, Val Loss: 3.4068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [39/100], Train Loss: 3.0393, Val Loss: 3.4008\n",
            "Checkpoint saved at epoch 39 with val loss 3.4008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [40/100], Train Loss: 3.0453, Val Loss: 3.4018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [41/100], Train Loss: 3.0399, Val Loss: 3.4003\n",
            "Checkpoint saved at epoch 41 with val loss 3.4003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [42/100], Train Loss: 3.0298, Val Loss: 3.3958\n",
            "Checkpoint saved at epoch 42 with val loss 3.3958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [43/100], Train Loss: 3.0171, Val Loss: 3.3923\n",
            "Checkpoint saved at epoch 43 with val loss 3.3923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [44/100], Train Loss: 3.0009, Val Loss: 3.3966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [45/100], Train Loss: 3.0078, Val Loss: 3.3965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [46/100], Train Loss: 3.0008, Val Loss: 3.3902\n",
            "Checkpoint saved at epoch 46 with val loss 3.3902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [47/100], Train Loss: 2.9989, Val Loss: 3.3831\n",
            "Checkpoint saved at epoch 47 with val loss 3.3831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [48/100], Train Loss: 2.9906, Val Loss: 3.3949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [49/100], Train Loss: 2.9754, Val Loss: 3.3917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/100], Train Loss: 2.9742, Val Loss: 3.3968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [51/100], Train Loss: 2.9484, Val Loss: 3.3882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [52/100], Train Loss: 2.9490, Val Loss: 3.3796\n",
            "Checkpoint saved at epoch 52 with val loss 3.3796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [53/100], Train Loss: 2.9389, Val Loss: 3.3839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [54/100], Train Loss: 2.9437, Val Loss: 3.3863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [55/100], Train Loss: 2.9328, Val Loss: 3.3795\n",
            "Checkpoint saved at epoch 55 with val loss 3.3795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [56/100], Train Loss: 2.9385, Val Loss: 3.3787\n",
            "Checkpoint saved at epoch 56 with val loss 3.3787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [57/100], Train Loss: 2.9272, Val Loss: 3.3799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [58/100], Train Loss: 2.9230, Val Loss: 3.3823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [59/100], Train Loss: 2.9220, Val Loss: 3.3792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [60/100], Train Loss: 2.9143, Val Loss: 3.3770\n",
            "Checkpoint saved at epoch 60 with val loss 3.3770\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [61/100], Train Loss: 2.9166, Val Loss: 3.3793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [62/100], Train Loss: 2.9157, Val Loss: 3.3793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [63/100], Train Loss: 2.9096, Val Loss: 3.3753\n",
            "Checkpoint saved at epoch 63 with val loss 3.3753\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [64/100], Train Loss: 2.9173, Val Loss: 3.3800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [65/100], Train Loss: 2.9141, Val Loss: 3.3769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [66/100], Train Loss: 2.9061, Val Loss: 3.3794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [67/100], Train Loss: 2.9137, Val Loss: 3.3808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [68/100], Train Loss: 2.9124, Val Loss: 3.3772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [69/100], Train Loss: 2.8996, Val Loss: 3.3772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [70/100], Train Loss: 2.9028, Val Loss: 3.3798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [71/100], Train Loss: 2.9037, Val Loss: 3.3782\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [72/100], Train Loss: 2.8987, Val Loss: 3.3774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [73/100], Train Loss: 2.8979, Val Loss: 3.3790\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [74/100], Train Loss: 2.9097, Val Loss: 3.3774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [75/100], Train Loss: 2.8946, Val Loss: 3.3753\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [76/100], Train Loss: 2.8945, Val Loss: 3.3786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [77/100], Train Loss: 2.8998, Val Loss: 3.3759\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [78/100], Train Loss: 2.9050, Val Loss: 3.3779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [79/100], Train Loss: 2.9014, Val Loss: 3.3797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [80/100], Train Loss: 2.9005, Val Loss: 3.3773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [81/100], Train Loss: 2.8994, Val Loss: 3.3777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [82/100], Train Loss: 2.8994, Val Loss: 3.3772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [83/100], Train Loss: 2.8990, Val Loss: 3.3799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [84/100], Train Loss: 2.8972, Val Loss: 3.3809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [85/100], Train Loss: 2.8934, Val Loss: 3.3774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [86/100], Train Loss: 2.9044, Val Loss: 3.3762\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [87/100], Train Loss: 2.9086, Val Loss: 3.3776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [88/100], Train Loss: 2.8983, Val Loss: 3.3768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [89/100], Train Loss: 2.9017, Val Loss: 3.3785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [90/100], Train Loss: 2.8866, Val Loss: 3.3757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [91/100], Train Loss: 2.9068, Val Loss: 3.3792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [92/100], Train Loss: 2.8874, Val Loss: 3.3784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [93/100], Train Loss: 2.9007, Val Loss: 3.3761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [94/100], Train Loss: 2.8988, Val Loss: 3.3772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [95/100], Train Loss: 2.9046, Val Loss: 3.3768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [96/100], Train Loss: 2.8911, Val Loss: 3.3777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [97/100], Train Loss: 2.9041, Val Loss: 3.3775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [98/100], Train Loss: 2.8932, Val Loss: 3.3781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [99/100], Train Loss: 2.8993, Val Loss: 3.3778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                                                       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/100], Train Loss: 2.9005, Val Loss: 3.3772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "# Hyperparameters (matching the original repo)\n",
        "embed_size    = 256\n",
        "hidden_size   = 512\n",
        "decoder_dim   = hidden_size   # decoder's LSTM hidden state dimension\n",
        "attention_dim = 256         # dimension for the attention module\n",
        "num_layers    = 1\n",
        "learning_rate = 3e-4\n",
        "num_epochs    = 100\n",
        "vocab_size    = len(vocab)\n",
        "\n",
        "# Initialize encoder and decoder models\n",
        "encoder = EncoderCNN(embed_size)\n",
        "decoder = DecoderRNNWithAttention(embed_size, decoder_dim, vocab_size, num_layers, attention_dim)\n",
        "\n",
        "# Define loss (ignoring the <PAD> token) and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"], label_smoothing=0.1)\n",
        "# Update only the decoder and the newly added layers of the encoder\n",
        "params = list(decoder.parameters()) + list(encoder.linear.parameters())\n",
        "optimizer = torch.optim.Adam(params, lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "# Start training\n",
        "train(encoder, decoder, criterion, optimizer, train_loader, val_loader, num_epochs, device, save_path=\"Downloads/Flickr/best_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from gtts import gTTS\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Load Trained Model Checkpoint\n",
        "# ----------------------------------------------------------------\n",
        "checkpoint_path = \"/content/drive/MyDrive/Flickr/best_model_100_attention.pth\"\n",
        "embed_size    = 256\n",
        "hidden_size   = 512\n",
        "decoder_dim   = hidden_size   # decoder's LSTM hidden state dimension\n",
        "attention_dim = 256         # dimension for the attention module\n",
        "num_layers    = 1\n",
        "learning_rate = 3e-4\n",
        "num_epochs    = 100\n",
        "vocab_size    = len(vocab)\n",
        "\n",
        "# Initialize encoder and decoder models\n",
        "encoder = EncoderCNN(embed_size)\n",
        "decoder = DecoderRNNWithAttention(embed_size, decoder_dim, vocab_size, num_layers, attention_dim)\n",
        "\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "encoder.eval()\n",
        "decoder.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYZC2fHWZSA3",
        "outputId": "a8c328f7-91b7-4393-dccd-984e5a7272ef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
            "100%|██████████| 104M/104M [00:01<00:00, 73.3MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecoderRNNWithAttention(\n",
              "  (embed): Embedding(3006, 256)\n",
              "  (attention): Attention(\n",
              "    (encoder_att): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (decoder_att): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (full_att): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (relu): ReLU()\n",
              "    (softmax): Softmax(dim=1)\n",
              "  )\n",
              "  (lstm): LSTM(512, 512, batch_first=True)\n",
              "  (linear): Linear(in_features=512, out_features=3006, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from gtts import gTTS\n",
        "import base64\n",
        "from io import BytesIO\n",
        "\n",
        "# [Your existing model loading code here...]\n",
        "\n",
        "def generate_captions_and_audio(files, beam_size, max_len=20):\n",
        "    \"\"\"Generate captions and audio for multiple images, returning HTML with embedded content\"\"\"\n",
        "    if not isinstance(files, list):\n",
        "        files = [files]\n",
        "\n",
        "    html_output = \"\"\n",
        "    with torch.no_grad():\n",
        "        for idx, file_path in enumerate(files):\n",
        "            # Process image\n",
        "            image = Image.open(file_path).convert(\"RGB\")\n",
        "            image_tensor = val_test_transform(image).unsqueeze(0).to(device)\n",
        "            features = encoder(image_tensor)\n",
        "\n",
        "            # Generate caption\n",
        "            token_ids = decoder.sample_beam(features, beam_size=beam_size, max_len=max_len)\n",
        "            caption_words = []\n",
        "            for token_id in token_ids:\n",
        "                word = vocab.itos[token_id]\n",
        "                if word == \"<EOS>\": break\n",
        "                if word not in [\"<SOS>\", \"<UNK>\"]:\n",
        "                    caption_words.append(word)\n",
        "            caption = \" \".join(caption_words)\n",
        "\n",
        "            # Generate audio\n",
        "            tts = gTTS(text=caption, lang=\"en\")\n",
        "            audio_bytes = BytesIO()\n",
        "            tts.write_to_fp(audio_bytes)\n",
        "            audio_bytes.seek(0)\n",
        "            audio_b64 = base64.b64encode(audio_bytes.read()).decode()\n",
        "\n",
        "            # Convert image to base64\n",
        "            buffered = BytesIO()\n",
        "            image.save(buffered, format=\"JPEG\")\n",
        "            img_b64 = base64.b64encode(buffered.getvalue()).decode()\n",
        "\n",
        "            # Create HTML block\n",
        "            html_output += f\"\"\"\n",
        "                        <div style=\"margin: 1rem; padding: 1rem; border: 1px solid #ddd; border-radius: 8px;\">\n",
        "                            <div style=\"display: flex; gap: 1rem; align-items: center;\">\n",
        "                                <img src=\"data:image/jpeg;base64,{img_b64}\" style=\"max-width: 200px; height: auto;\"/>\n",
        "                                <div>\n",
        "                                    <p style=\"font-size: 18px; margin: 0.5rem 0;\">\n",
        "                                        <strong style=\"font-size: 16px;\">Caption:</strong>\n",
        "                                        <span style=\"font-size: 16px;\">{caption}</span>\n",
        "                                    </p>\n",
        "                                    <audio controls style=\"margin-top: 0.5rem;\">\n",
        "                                        <source src=\"data:audio/mpeg;base64,{audio_b64}\" type=\"audio/mpeg\">\n",
        "                                    </audio>\n",
        "                                </div>\n",
        "                            </div>\n",
        "                      </div>\n",
        "                      \"\"\"\n",
        "\n",
        "    return f\"<div style='margin: 1rem;'>{html_output}</div>\"\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(title=\"Image Captioning with Audio\") as interface:\n",
        "    gr.Markdown(\"# Image Captioning with Audio\")\n",
        "    gr.Markdown(\"Upload Images to Generate Captions with Playable Audio. (Up to 10 Images)\")\n",
        "    gr.Markdown(\"Model used is InceptionV3 and Training Dataset is Flickr8k.\")\n",
        "    gr.Markdown(\"Beam Search is used as the Decoding Strategy.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        inputs = [\n",
        "            gr.File(label=\"Upload Images\", file_count=\"multiple\", file_types=[\"image\"], type=\"filepath\"),\n",
        "            gr.Slider(1, 10, step=1, value=3, label=\"Beam Size\"),\n",
        "            gr.Slider(10, 50, step=1, value=20, label=\"Max Caption Length\")\n",
        "        ]\n",
        "        submit = gr.Button(\"Generate\", variant=\"primary\")\n",
        "\n",
        "    output = gr.HTML(label=\"Results\")\n",
        "\n",
        "    # Validation function\n",
        "    def validate_files(files):\n",
        "        if len(files) < 1:\n",
        "            raise gr.Error(\"Please upload at least 1 image!\")\n",
        "        if len(files) > 10:\n",
        "            raise gr.Error(\"Maximum 10 images allowed!\")\n",
        "        return files\n",
        "\n",
        "        # Add validation to file input\n",
        "    inputs[0].upload(\n",
        "        validate_files,\n",
        "        inputs[0],\n",
        "        inputs[0]\n",
        "    )\n",
        "\n",
        "    submit.click(\n",
        "        generate_captions_and_audio,\n",
        "        inputs=inputs,\n",
        "        outputs=output\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "KdJdxg0MZkjy",
        "outputId": "2675e033-e283-406a-984e-785f6170a982"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5bf6e6e5347bd3cff4.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5bf6e6e5347bd3cff4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0zmBxiODqttP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_CwKd6lZqtoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-xtljmbhqtjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sjqGZuTWqtf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_dIry9XCqtaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bayesian-optimization"
      ],
      "metadata": {
        "id": "F5wmpx9Av78M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bd08a78-2b15-4877-eec6-974b30eeae84",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian_optimization-2.0.3-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.6 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bayesian-optimization) (0.4.6)\n",
            "Collecting numpy>=1.25 (from bayesian-optimization)\n",
            "  Downloading numpy-2.2.4-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.0.0 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bayesian-optimization) (1.4.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.0.0 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bayesian-optimization) (1.13.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn<2.0.0,>=1.0.0->bayesian-optimization) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn<2.0.0,>=1.0.0->bayesian-optimization) (3.5.0)\n",
            "Downloading bayesian_optimization-2.0.3-py3-none-any.whl (31 kB)\n",
            "Downloading numpy-2.2.4-cp310-cp310-win_amd64.whl (12.9 MB)\n",
            "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
            "   ----- ---------------------------------- 1.8/12.9 MB 10.1 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 4.7/12.9 MB 13.0 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 7.6/12.9 MB 13.4 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 10.0/12.9 MB 12.9 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 11.8/12.9 MB 12.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 12.9/12.9 MB 11.6 MB/s eta 0:00:00\n",
            "Installing collected packages: numpy, bayesian-optimization\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.3\n",
            "    Uninstalling numpy-1.24.3:\n",
            "      Successfully uninstalled numpy-1.24.3\n",
            "Successfully installed bayesian-optimization-2.0.3 numpy-2.2.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Pushkar\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\~umpy'.\n",
            "  You can safely remove it manually.\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.59.1 requires numpy<1.27,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow-intel 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.12.2 which is incompatible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "def train_and_evaluate(learning_rate, hidden_size, dropout_rate, num_epochs):\n",
        "    # Convert hyperparameters to proper types\n",
        "    hidden_size = int(hidden_size)\n",
        "    num_epochs = int(num_epochs)\n",
        "    dropout_rate = max(min(dropout_rate, 1), 0)  # ensure dropout is in [0,1]\n",
        "\n",
        "    # Fixed parameters\n",
        "    embed_size = 256\n",
        "    num_layers = 1\n",
        "    attention_dim = 256\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    # Initialize models using the given hyperparameters\n",
        "    encoder_model = EncoderCNN(embed_size)\n",
        "    decoder_model = DecoderRNNWithAttention(embed_size, hidden_size, vocab_size, num_layers, attention_dim)\n",
        "    # Adjust dropout if your decoder has a dropout layer attribute\n",
        "    decoder_model.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    encoder_model.to(device)\n",
        "    decoder_model.to(device)\n",
        "\n",
        "    # Define loss and optimizer using the current learning rate\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])\n",
        "    params = list(decoder_model.parameters()) + list(encoder_model.linear.parameters()) + list(encoder_model.bn.parameters())\n",
        "    optimizer_model = torch.optim.Adam(params, lr=learning_rate)\n",
        "\n",
        "    # Train for the given number of epochs\n",
        "    train(encoder_model, decoder_model, criterion, optimizer_model, train_loader, val_loader, num_epochs, device, save_path=\"temp_model.pth\")\n",
        "\n",
        "    # Evaluate on the validation set (we aim to minimize loss, so we return negative loss for maximization)\n",
        "    val_loss = evaluate(encoder_model, decoder_model, criterion, val_loader, device)\n",
        "\n",
        "    # Return negative validation loss (so higher is better)\n",
        "    return -val_loss\n",
        "\n",
        "# Define the search space for each hyperparameter, including num_epochs\n",
        "pbounds = {\n",
        "    'learning_rate': (1e-5, 1e-3),  # Learning rate between 0.00001 and 0.001\n",
        "    'hidden_size': (256, 1024),       # LSTM hidden size between 256 and 1024\n",
        "    'dropout_rate': (0.0, 0.5),       # Dropout between 0 and 0.5\n",
        "    'num_epochs': (3, 10)             # Number of epochs between 3 and 10 (as a continuous parameter, cast to int)\n",
        "}\n",
        "\n",
        "optimizer_bo = BayesianOptimization(\n",
        "    f=train_and_evaluate,\n",
        "    pbounds=pbounds,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# Run optimization: for example, 5 random initial points and 10 iterations\n",
        "optimizer_bo.maximize(\n",
        "    init_points=5,\n",
        "    n_iter=10,\n",
        ")\n",
        "\n",
        "print(\"Best hyperparameters found:\")\n",
        "print(optimizer_bo.max)"
      ],
      "metadata": {
        "id": "P9C4GvK6vzQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Load the best model checkpoint and evaluate sacreBLEU score on the test set\n",
        "# also model loading code for gradio UI and HF Spaces Deployment\n",
        "\n",
        "# Load checkpoint\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/Flickr/best_model.pth\", map_location=torch.device('cpu'))\n",
        "\n",
        "# Instantiate your models with the same architecture and hyperparameters\n",
        "encoder = EncoderCNN(embed_size)\n",
        "decoder = DecoderRNNWithAttention(embed_size, decoder_dim, vocab_size, num_layers, attention_dim)\n",
        "\n",
        "# Load the saved weights into the models\n",
        "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
        "\n",
        "# Move the models to the correct device and set them to evaluation mode\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "encoder.eval()\n",
        "decoder.eval()"
      ],
      "metadata": {
        "id": "A0mHkOhzlwzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlwCW5z3g__p"
      },
      "outputs": [],
      "source": [
        "# Compute and print the BLEU score using the test loader\n",
        "bleu_score = compute_bleu_score(encoder, decoder, test_loader, vocab, device)\n",
        "print(\"Test sacreBLEU score: {:.2f}\".format(bleu_score))\n",
        "\n",
        "# Compute METEOR score\n",
        "meteor = compute_meteor_score(encoder, decoder, test_loader, vocab, device)\n",
        "print(\"Test METEOR score: {:.4f}\".format(meteor))\n",
        "\n",
        "# Compute CIDEr score\n",
        "cider = compute_cider_score(encoder, decoder, test_loader, vocab, device)\n",
        "print(\"Test CIDEr score: {:.4f}\".format(cider))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Folder path containing test images\n",
        "folder_path = \"/content/drive/MyDrive/Flickr/test_images/\"\n",
        "\n",
        "# Dictionary to store captions for each file\n",
        "captions_dict = {}\n",
        "\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "            image_path = os.path.join(folder_path, filename)\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            image_tensor = val_test_transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "            features = encoder(image_tensor)\n",
        "            # Use beam search for caption generation; you can switch to greedy with decoder.sample(features)\n",
        "            best_seq = decoder.sample_beam(features, beam_size=3, max_len=20)\n",
        "\n",
        "            # Convert token IDs to words, filtering out special tokens\n",
        "            caption_words = []\n",
        "            for token_id in best_seq:\n",
        "                word = vocab.itos[token_id]\n",
        "                if word == \"<EOS>\":\n",
        "                    break\n",
        "                if word in [\"<SOS>\", \"<UNK>\"]:\n",
        "                    continue\n",
        "                caption_words.append(word)\n",
        "            caption = \" \".join(caption_words)\n",
        "            captions_dict[filename] = caption\n",
        "            print(f\"{filename}: {caption}\")\n",
        "\n",
        "# Optionally, captions_dict now holds the mapping for further processing."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Im5U_UjjMYD",
        "outputId": "15c8b0c7-5e21-41be-bd53-c1c0c9889afd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boat.png: a boat is in the water .\n",
            "bus.png: a man in a white shirt is standing in front of a bus .\n",
            "child.jpg: a little boy in a red shirt is playing with a toy .\n",
            "dog.jpg: a brown dog is running on the beach .\n",
            "horse.png: a man and a dog are standing on a beach .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference Cell: Generate a caption for a sample test image using Beam Search with Attention\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "# Update with the path to your sample test image\n",
        "sample_image_path = \"/content/drive/MyDrive/Flickr/test_images/dog.jpg\"\n",
        "sample_image = Image.open(sample_image_path).convert(\"RGB\")\n",
        "sample_image = val_test_transform(sample_image).unsqueeze(0).to(device)\n",
        "\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Obtain spatial features from the encoder; shape: (1, num_pixels, embed_size)\n",
        "    features = encoder(sample_image)\n",
        "    # Use beam search to generate a caption (adjust beam_size and max_len as needed)\n",
        "    best_seq = decoder.sample_beam(features, beam_size=3, max_len=20)\n",
        "\n",
        "# Convert the sequence of token IDs to words, skipping unwanted tokens\n",
        "caption_words = []\n",
        "for token_id in best_seq:\n",
        "    word = vocab.itos[token_id]\n",
        "    if word == \"<EOS>\":\n",
        "        break\n",
        "    if word in [\"<SOS>\", \"<UNK>\"]:\n",
        "        continue\n",
        "    caption_words.append(word)\n",
        "caption = \" \".join(caption_words)\n",
        "print(\"Generated Caption (Beam Search with Attention):\", caption)"
      ],
      "metadata": {
        "id": "tZgYu7udgyJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9c4ec86-188a-4ebc-fd44-07b3393e54a4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Caption (Beam Search with Attention): a brown dog is running on the beach .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gtts import gTTS\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# Suppose 'caption' is the generated caption from your inference cell.\n",
        "# If it's not defined, set it manually for testing.\n",
        "# caption = \"A child in a pink dress is climbing up a set of stairs in an entry way.\"\n",
        "\n",
        "# Convert text to speech\n",
        "tts = gTTS(text=caption, lang='en')\n",
        "\n",
        "# Save the audio file\n",
        "audio_filename = \"caption_audio.mp3\"\n",
        "tts.save(audio_filename)\n",
        "\n",
        "# Play the audio in the notebook\n",
        "display(Audio(audio_filename, autoplay=True))"
      ],
      "metadata": {
        "id": "fZAHDWdDhRtR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "16f90f3c-fd76-4bb0-8a82-2216ed4f34d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ],
            "text/html": [
              "\n",
              "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
              "                    <source src=\"data:audio/mpeg;base64,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\" type=\"audio/mpeg\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from gtts import gTTS\n",
        "import torch\n",
        "\n",
        "def generate_caption_and_audio(image):\n",
        "    \"\"\"\n",
        "    Accepts an image (PIL), preprocesses it, runs inference using the attention-enabled decoder\n",
        "    with beam search, converts the predicted tokens to a caption (excluding <SOS> and <UNK>),\n",
        "    and converts the caption to audio.\n",
        "\n",
        "    Returns:\n",
        "        caption (str): Generated caption text.\n",
        "        audio_file (str): Path to the generated audio file (MP3).\n",
        "    \"\"\"\n",
        "    # Ensure the image is RGB and preprocessed\n",
        "    image = image.convert(\"RGB\")\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Set models to eval mode (redundant if already done globally)\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Obtain encoder spatial features and run beam search decoding\n",
        "        features = encoder(image_tensor)\n",
        "        best_seq = decoder.sample_beam(features, beam_size=3, max_len=20)\n",
        "\n",
        "    # Convert token IDs to words and filter unwanted tokens\n",
        "    caption_words = []\n",
        "    for token_id in best_seq:\n",
        "        word = vocab.itos[token_id]\n",
        "        if word == \"<EOS>\":\n",
        "            break\n",
        "        if word in [\"<SOS>\", \"<UNK>\"]:\n",
        "            continue\n",
        "        caption_words.append(word)\n",
        "    caption = \" \".join(caption_words)\n",
        "\n",
        "    # Convert the caption text to speech using gTTS and save to an MP3 file\n",
        "    tts = gTTS(text=caption, lang='en')\n",
        "    audio_file = \"caption_audio.mp3\"\n",
        "    tts.save(audio_file)\n",
        "\n",
        "    return caption, audio_file\n",
        "\n",
        "# Create a Gradio Interface with an image input and text + audio outputs.\n",
        "interface = gr.Interface(\n",
        "    fn=generate_caption_and_audio,\n",
        "    inputs=gr.Image(type=\"pil\", label=\"Input Image\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Generated Caption\"),\n",
        "        gr.Audio(label=\"Caption Audio\")\n",
        "    ],\n",
        "    title=\"Image Captioning with Audio Output\",\n",
        "    description=\"Upload an image to get a generated caption (text) along with an audio version of the caption.\"\n",
        ")\n",
        "\n",
        "interface.launch()"
      ],
      "metadata": {
        "id": "PRtO6Z3Uo6Fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bleu_score(encoder, decoder, data_loader, vocab, device):\n",
        "    \"\"\"\n",
        "    Computes sacreBLEU score using all available reference captions for each image.\n",
        "\n",
        "    Assumes that:\n",
        "      - data_loader.dataset.img2caps maps image ids to a list of reference captions.\n",
        "      - data_loader.dataset.imgs is an ordered list of image ids.\n",
        "      - The DataLoader is used with shuffle=False.\n",
        "    \"\"\"\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    all_hypotheses = []\n",
        "    all_references = []  # list of lists of reference captions, one per image\n",
        "    global_idx = 0      # index to track image id order in dataset\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, _ in data_loader:\n",
        "            images = images.to(device)\n",
        "            features = encoder(images)\n",
        "            batch_size = images.size(0)\n",
        "            for i in range(batch_size):\n",
        "                feature = features[i].unsqueeze(0)\n",
        "                # Generate caption using greedy decoding (or replace with beam search if desired)\n",
        "                sampled_ids = decoder.sample(feature)\n",
        "                sampled_caption = []\n",
        "                for token_id in sampled_ids:\n",
        "                    word = vocab.itos[token_id]\n",
        "                    if word == \"<EOS>\":\n",
        "                        break\n",
        "                    if word in [\"<SOS>\", \"<UNK>\"]:\n",
        "                        continue\n",
        "                    sampled_caption.append(word)\n",
        "                hypothesis = \" \".join(sampled_caption)\n",
        "                all_hypotheses.append(hypothesis)\n",
        "\n",
        "                # Retrieve all reference captions for the current image\n",
        "                image_id = data_loader.dataset.imgs[global_idx]\n",
        "                references_for_image = data_loader.dataset.img2caps[image_id]\n",
        "                all_references.append(references_for_image)\n",
        "                global_idx += 1\n",
        "\n",
        "    bleu = sacrebleu.corpus_bleu(all_hypotheses, all_references)\n",
        "    return bleu.score"
      ],
      "metadata": {
        "id": "6eeew_-53GjC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}